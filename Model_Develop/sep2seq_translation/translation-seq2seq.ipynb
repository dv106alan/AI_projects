{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 16:02:21.766136: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3 #20\n",
    "BATCH_SIZE = 128\n",
    "MAX_WORDS = 10000\n",
    "READ_LINES = 60000\n",
    "LAYER_SIZE = 256\n",
    "EMBEDDING_WIDTH = 128\n",
    "TEST_PERCENT = 0.2\n",
    "SAMPLE_SIZE = 20\n",
    "OOV_WORD = 'UNK'\n",
    "PAD_INDEX = 0\n",
    "OOV_INDEX = 1\n",
    "START_INDEX = MAX_WORDS - 2\n",
    "STOP_INDEX = MAX_WORDS - 1\n",
    "MAX_LENGTH = 60\n",
    "SRC_DEST_FILE_NAME = './data/fra.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立訓練資料處理函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_combined(file_name, max_len):\n",
    "    file = open(file_name, 'r', encoding='utf-8')\n",
    "    src_word_sequences = []\n",
    "    dest_word_sequences = []\n",
    "    for i, line in enumerate(file):\n",
    "        if i == READ_LINES:\n",
    "            break\n",
    "        pair = line.split('\\t')\n",
    "        word_sequence = text_to_word_sequence(pair[1])\n",
    "        src_word_sequence = word_sequence[0:max_len]\n",
    "        src_word_sequences.append(src_word_sequence)\n",
    "        \n",
    "        word_sequence = text_to_word_sequence(pair[0])\n",
    "        dest_word_sequence = word_sequence[0:max_len]\n",
    "        dest_word_sequences.append(dest_word_sequence)\n",
    "    \n",
    "    file.close()\n",
    "    return src_word_sequences, dest_word_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sequences):\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS-2, oov_token=OOV_WORD)\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    token_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    return tokenizer, token_sequences\n",
    "\n",
    "def tokens_to_words(tokenizer, seq):\n",
    "    word_seq = []\n",
    "    for index in seq:\n",
    "        if index == PAD_INDEX:\n",
    "            word_seq.append('PAD')\n",
    "        elif index == OOV_INDEX:\n",
    "            word_seq.append(OOV_WORD)\n",
    "        elif index == START_INDEX:\n",
    "            word_seq.append('START')\n",
    "        elif index == STOP_INDEX:\n",
    "            word_seq.append('STOP')\n",
    "        else:\n",
    "            word_seq.append(tokenizer.sequences_to_texts([[index]])[0])\n",
    "    print(word_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_seq, dest_seq = read_file_combined(SRC_DEST_FILE_NAME, MAX_LENGTH)\n",
    "src_tokenizer, src_token_seq = tokenize(src_seq)\n",
    "dest_tokenizer, dest_token_seq = tokenize(dest_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_target_token_seq = [x + [STOP_INDEX] for x in dest_token_seq]\n",
    "dest_input_token_seq = [[START_INDEX] + x for x in dest_target_token_seq]\n",
    "\n",
    "src_input_data = pad_sequences(src_token_seq)\n",
    "dest_input_data = pad_sequences(dest_input_token_seq, padding='post')\n",
    "\n",
    "dest_target_data = pad_sequences(dest_target_token_seq, padding='post', maxlen=len(dest_input_data[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quel', 'fiasco']\n",
      "['what', 'a', 'fiasco']\n"
     ]
    }
   ],
   "source": [
    "print(src_seq[9999])\n",
    "print(dest_seq[9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  35    5 3807 9999    0    0    0    0    0]\n",
      "[9998   35    5 3807 9999    0    0    0    0]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0  136 6226]\n"
     ]
    }
   ],
   "source": [
    "print(dest_target_data[9999])\n",
    "print(dest_input_data[9999])\n",
    "print(src_input_data[9999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切割訓練測試資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = len(src_input_data[:,0])\n",
    "all_indices = list(range(rows))\n",
    "test_rows = int(rows * TEST_PERCENT)\n",
    "test_indices = random.sample(all_indices, test_rows)\n",
    "train_indices = [x for x in all_indices if x not in test_indices]\n",
    "\n",
    "train_src_input_data = src_input_data[train_indices]\n",
    "train_dest_input_data = dest_input_data[train_indices]\n",
    "train_dest_target_data = dest_target_data[train_indices]\n",
    "\n",
    "test_src_input_data = src_input_data[test_indices]\n",
    "test_dest_input_data = dest_input_data[test_indices]\n",
    "test_dest_target_data = dest_target_data[test_indices]\n",
    "\n",
    "test_indices = list(range(test_rows))\n",
    "sample_indices = random.sample(test_indices, SAMPLE_SIZE)\n",
    "sample_input_data = test_src_input_data[sample_indices]\n",
    "sample_target_data = test_dest_target_data[sample_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建構模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 128)         1280000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               [(None, None, 256),       394240    \n",
      "                              (None, 256),                       \n",
      "                              (None, 256)]                       \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               [(None, 256),             525312    \n",
      "                              (None, 256),                       \n",
      "                              (None, 256)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,199,552\n",
      "Trainable params: 2,199,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 編碼器模型\n",
    "enc_embedding_input = Input(shape=(None,))\n",
    "\n",
    "enc_embedding_layer = Embedding(output_dim=EMBEDDING_WIDTH, input_dim=MAX_WORDS, mask_zero=True)\n",
    "enc_layer1 = LSTM(LAYER_SIZE, return_state=True, return_sequences=True)\n",
    "enc_layer2 = LSTM(LAYER_SIZE, return_state=True)\n",
    "\n",
    "enc_embedding_layer_outputs = enc_embedding_layer(enc_embedding_input)\n",
    "enc_layer1_outputs, enc_layer1_state_h, enc_layer1_state_c = enc_layer1(enc_embedding_layer_outputs) \n",
    "_, enc_layer2_state_h, enc_layer2_state_c = enc_layer2(enc_layer1_outputs)\n",
    "\n",
    "enc_model = Model(enc_embedding_input, [enc_layer1_state_h, enc_layer1_state_c,\n",
    "                                        enc_layer2_state_h, enc_layer2_state_c])\n",
    "\n",
    "enc_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 128)    1280000     ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  [(None, None, 256),  394240      ['embedding_2[0][0]',            \n",
      "                                 (None, 256),                     'input_3[0][0]',                \n",
      "                                 (None, 256)]                     'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  [(None, None, 256),  525312      ['lstm_4[0][0]',                 \n",
      "                                 (None, 256),                     'input_5[0][0]',                \n",
      "                                 (None, 256)]                     'input_6[0][0]',                \n",
      "                                                                  'lstm_5[0][0]',                 \n",
      "                                                                  'lstm_5[0][1]',                 \n",
      "                                                                  'lstm_5[0][2]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 10000)  2570000     ['lstm_5[1][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,769,552\n",
      "Trainable params: 4,769,552\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 解碼器模型\n",
    "dec_layer1_state_input_h = Input(shape=(LAYER_SIZE,))\n",
    "dec_layer1_state_input_c = Input(shape=(LAYER_SIZE,))\n",
    "dec_layer2_state_input_h = Input(shape=(LAYER_SIZE,))\n",
    "dec_layer2_state_input_c = Input(shape=(LAYER_SIZE,))\n",
    "dec_embedding_input = Input(shape=(None,))\n",
    "\n",
    "dec_embedding_layer = Embedding(output_dim=EMBEDDING_WIDTH, input_dim=MAX_WORDS, mask_zero=True)\n",
    "dec_layer1 = LSTM(LAYER_SIZE, return_state=True, return_sequences=True)\n",
    "dec_layer2 = LSTM(LAYER_SIZE, return_state=True, return_sequences=True)\n",
    "dec_layer3 = Dense(MAX_WORDS, activation='softmax')\n",
    "\n",
    "dec_embedding_layer_output = dec_embedding_layer(dec_embedding_input)\n",
    "dec_layer1_outputs, dec_layer1_state_h, dec_layer1_state_c = dec_layer1(\n",
    "    dec_embedding_layer_output, initial_state=[\n",
    "        dec_layer1_state_input_h, dec_layer1_state_input_c])\n",
    "dec_layer2_outputs, dec_layer2_state_h, dec_layer2_state_c = dec_layer2(\n",
    "    dec_layer2(dec_layer1_outputs, initial_state=[\n",
    "        dec_layer2_state_input_h, dec_layer2_state_input_c]))\n",
    "dec_layer3_output = dec_layer3(dec_layer2_outputs)\n",
    "\n",
    "dec_model = Model([dec_embedding_input, \n",
    "                   dec_layer1_state_input_h, \n",
    "                   dec_layer1_state_input_c,\n",
    "                   dec_layer2_state_input_h, \n",
    "                   dec_layer2_state_input_c],\n",
    "                  [dec_layer3_output, \n",
    "                   dec_layer1_state_h, dec_layer1_state_c,\n",
    "                   dec_layer2_state_h, dec_layer2_state_c])\n",
    "dec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " model (Functional)             [(None, 256),        2199552     ['input_10[0][0]']               \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " model_1 (Functional)           [(None, None, 10000  4769552     ['input_11[0][0]',               \n",
      "                                ),                                'model[1][0]',                  \n",
      "                                 (None, 256),                     'model[1][1]',                  \n",
      "                                 (None, 256),                     'model[1][2]',                  \n",
      "                                 (None, 256),                     'model[1][3]']                  \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,969,104\n",
      "Trainable params: 6,969,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 建構並編譯整個訓練模型\n",
    "train_enc_embedding_input = Input(shape=(None, ))\n",
    "train_dec_embedding_input = Input(shape=(None, ))\n",
    "intermediate_state = enc_model(train_enc_embedding_input)\n",
    "train_dec_output, _, _, _, _ = dec_model(\n",
    "    [train_dec_embedding_input] +\n",
    "    intermediate_state)\n",
    "training_model = Model([train_enc_embedding_input,\n",
    "                        train_dec_embedding_input],\n",
    "                        train_dec_output)\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "training_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer=optimizer, metrics =['accuracy'])\n",
    "training_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "1500/1500 [==============================] - 288s 192ms/step - loss: 1.9618 - accuracy: 0.6966 - val_loss: 1.8122 - val_accuracy: 0.7220\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'personne', 'ne', \"l'a\", 'su']\n",
      "['no', 'one', 'knew', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'attends', 'une', 'minute']\n",
      "['wait', 'a', 'minute', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dis', 'toujours', 'la', 'vérité']\n",
      "['always', 'tell', 'the', 'truth', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"quelqu'un\", 'a', 'pris', 'ma', 'place']\n",
      "['someone', 'took', 'my', 'place', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'était', 'lui', 'aussi', 'excité']\n",
      "['tom', 'was', 'excited', 'too', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'donnez', 'le', 'moi']\n",
      "['give', 'it', 'to', 'me', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'avec', 'qui', 'parle', 't', 'il\\u202f']\n",
      "['who', 'is', 'he', 'talking', 'to', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'en', 'as', 'tu', 'une', 'copie']\n",
      "['do', 'you', 'have', 'a', 'copy', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'où', 'nous', 'rencontrerons', 'nous']\n",
      "['where', 'will', 'we', 'meet', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', 'le', 'frappa', 'à', 'la', 'tête']\n",
      "['he', 'banged', 'his', 'head', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'c’est', 'très', 'bon']\n",
      "[\"that's\", 'very', 'good', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'arrête', 'de', 'chanter', 'je', 'te', 'prie']\n",
      "['please', 'stop', 'singing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'nous', 'tenterons']\n",
      "[\"we'll\", 'try', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'un', 'nom', 'inhabituel']\n",
      "[\"it's\", 'an', 'unusual', 'name', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'le', 'dirai', 'à', 'ma', 'femme']\n",
      "[\"i'll\", 'tell', 'my', 'wife', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', \"n'est\", 'pas', 'tout', 'à', 'fait', 'là']\n",
      "[\"he's\", 'not', 'all', 'there', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ne', 'sois', 'pas', 'alarmé']\n",
      "[\"don't\", 'be', 'alarmed', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"qu'y\", 'a', 't', 'il\\u202f']\n",
      "['what', 'is', 'going', 'on', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'entre']\n",
      "['come', 'in', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', 'y', 'a', 'une', 'pièce', \"d'or\"]\n",
      "['there', 'is', 'a', 'gold', 'coin', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  1\n",
      "1500/1500 [==============================] - 295s 197ms/step - loss: 1.6093 - accuracy: 0.7520 - val_loss: 1.5250 - val_accuracy: 0.7716\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'personne', 'ne', \"l'a\", 'su']\n",
      "['no', 'one', 'knew', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'attends', 'une', 'minute']\n",
      "['wait', 'a', 'minute', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dis', 'toujours', 'la', 'vérité']\n",
      "['always', 'tell', 'the', 'truth', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"quelqu'un\", 'a', 'pris', 'ma', 'place']\n",
      "['someone', 'took', 'my', 'place', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'était', 'lui', 'aussi', 'excité']\n",
      "['tom', 'was', 'excited', 'too', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'donnez', 'le', 'moi']\n",
      "['give', 'it', 'to', 'me', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'avec', 'qui', 'parle', 't', 'il\\u202f']\n",
      "['who', 'is', 'he', 'talking', 'to', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'en', 'as', 'tu', 'une', 'copie']\n",
      "['do', 'you', 'have', 'a', 'copy', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'où', 'nous', 'rencontrerons', 'nous']\n",
      "['where', 'will', 'we', 'meet', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', 'le', 'frappa', 'à', 'la', 'tête']\n",
      "['he', 'banged', 'his', 'head', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'c’est', 'très', 'bon']\n",
      "[\"that's\", 'very', 'good', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'arrête', 'de', 'chanter', 'je', 'te', 'prie']\n",
      "['please', 'stop', 'singing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'nous', 'tenterons']\n",
      "[\"we'll\", 'try', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'un', 'nom', 'inhabituel']\n",
      "[\"it's\", 'an', 'unusual', 'name', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'le', 'dirai', 'à', 'ma', 'femme']\n",
      "[\"i'll\", 'tell', 'my', 'wife', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', \"n'est\", 'pas', 'tout', 'à', 'fait', 'là']\n",
      "[\"he's\", 'not', 'all', 'there', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ne', 'sois', 'pas', 'alarmé']\n",
      "[\"don't\", 'be', 'alarmed', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"qu'y\", 'a', 't', 'il\\u202f']\n",
      "['what', 'is', 'going', 'on', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'entre']\n",
      "['come', 'in', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', 'y', 'a', 'une', 'pièce', \"d'or\"]\n",
      "['there', 'is', 'a', 'gold', 'coin', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  2\n",
      "1500/1500 [==============================] - 292s 194ms/step - loss: 1.3373 - accuracy: 0.7957 - val_loss: 1.3083 - val_accuracy: 0.8042\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'personne', 'ne', \"l'a\", 'su']\n",
      "['no', 'one', 'knew', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'attends', 'une', 'minute']\n",
      "['wait', 'a', 'minute', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dis', 'toujours', 'la', 'vérité']\n",
      "['always', 'tell', 'the', 'truth', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"quelqu'un\", 'a', 'pris', 'ma', 'place']\n",
      "['someone', 'took', 'my', 'place', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'était', 'lui', 'aussi', 'excité']\n",
      "['tom', 'was', 'excited', 'too', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'donnez', 'le', 'moi']\n",
      "['give', 'it', 'to', 'me', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'avec', 'qui', 'parle', 't', 'il\\u202f']\n",
      "['who', 'is', 'he', 'talking', 'to', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'en', 'as', 'tu', 'une', 'copie']\n",
      "['do', 'you', 'have', 'a', 'copy', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'où', 'nous', 'rencontrerons', 'nous']\n",
      "['where', 'will', 'we', 'meet', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', 'le', 'frappa', 'à', 'la', 'tête']\n",
      "['he', 'banged', 'his', 'head', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'c’est', 'très', 'bon']\n",
      "[\"that's\", 'very', 'good', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'arrête', 'de', 'chanter', 'je', 'te', 'prie']\n",
      "['please', 'stop', 'singing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'nous', 'tenterons']\n",
      "[\"we'll\", 'try', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'un', 'nom', 'inhabituel']\n",
      "[\"it's\", 'an', 'unusual', 'name', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'le', 'dirai', 'à', 'ma', 'femme']\n",
      "[\"i'll\", 'tell', 'my', 'wife', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', \"n'est\", 'pas', 'tout', 'à', 'fait', 'là']\n",
      "[\"he's\", 'not', 'all', 'there', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ne', 'sois', 'pas', 'alarmé']\n",
      "[\"don't\", 'be', 'alarmed', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"qu'y\", 'a', 't', 'il\\u202f']\n",
      "['what', 'is', 'going', 'on', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'entre']\n",
      "['come', 'in', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', 'y', 'a', 'une', 'pièce', \"d'or\"]\n",
      "['there', 'is', 'a', 'gold', 'coin', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "['STOP']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 訓練並測試模型\n",
    "for i in range(EPOCHS):\n",
    "    print('Epoch: ' , i)\n",
    "    # 訓練一個週期\n",
    "    history = training_model.fit(\n",
    "        [train_src_input_data, train_dest_input_data], train_dest_target_data, \n",
    "        validation_data=([test_src_input_data, test_dest_input_data], test_dest_target_data), \n",
    "        epochs=1)\n",
    "    # 將測試樣本送入模型\n",
    "    for (test_input, test_target) in zip(sample_input_data, sample_target_data):\n",
    "        x = np.reshape(test_input, (1,-1))\n",
    "        last_states = enc_model.predict(x, verbose=0)\n",
    "        prev_word_index = START_INDEX\n",
    "        produced_string = ''\n",
    "        pred_seq = []\n",
    "        for j in range(MAX_LENGTH):\n",
    "            x = np.reshape(np.array(prev_word_index), (1,-1))\n",
    "            preds, dec_layer1_state_h, dec_layer1_state_c, dec_layer2_state_h, dec_layer2_state_c = dec_model.predict([x] + last_states, verbose=0)\n",
    "            last_states = [dec_layer1_state_h, dec_layer1_state_c, \n",
    "                           dec_layer2_state_h, dec_layer2_state_c]\n",
    "            # 挑出可能性最高單字\n",
    "            prev_word_index = np.asarray(preds[0][0]).argmax()\n",
    "            pred_seq.append(prev_word_index)\n",
    "            if prev_word_index == STOP_INDEX:\n",
    "                break\n",
    "        tokens_to_words(src_tokenizer, test_input)\n",
    "        tokens_to_words(dest_tokenizer, test_target)\n",
    "        tokens_to_words(dest_tokenizer, pred_seq)\n",
    "        print('\\n\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3-8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
