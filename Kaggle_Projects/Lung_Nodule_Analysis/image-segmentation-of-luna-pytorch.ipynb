{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LUNA - CT Image Segmentation\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Install Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:54:49.558493Z","iopub.status.busy":"2024-07-05T04:54:49.557513Z","iopub.status.idle":"2024-07-05T04:55:03.210718Z","shell.execute_reply":"2024-07-05T04:55:03.209494Z","shell.execute_reply.started":"2024-07-05T04:54:49.558449Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting diskcache\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: diskcache\n","Successfully installed diskcache-5.6.3\n"]}],"source":["!pip install diskcache"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:03.212857Z","iopub.status.busy":"2024-07-05T04:55:03.212546Z","iopub.status.idle":"2024-07-05T04:55:03.217608Z","shell.execute_reply":"2024-07-05T04:55:03.216760Z","shell.execute_reply.started":"2024-07-05T04:55:03.212827Z"},"trusted":true},"outputs":[],"source":["import collections\n","import copy\n","import datetime\n","import gc\n","import time\n","import sys\n","import math\n","import random\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:08.522615Z","iopub.status.busy":"2024-07-05T04:55:08.521773Z","iopub.status.idle":"2024-07-05T04:55:08.530729Z","shell.execute_reply":"2024-07-05T04:55:08.529603Z","shell.execute_reply.started":"2024-07-05T04:55:08.522582Z"},"trusted":true},"outputs":[],"source":["IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\n","XyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n","\n","def irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n","    cri_a = np.array(coord_irc)[::-1]\n","    origin_a = np.array(origin_xyz)\n","    vxSize_a = np.array(vxSize_xyz)\n","    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a\n","    return XyzTuple(*coords_xyz)\n","\n","def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n","    origin_a = np.array(origin_xyz)\n","    vxSize_a = np.array(vxSize_xyz)\n","    coord_a = np.array(coord_xyz)\n","    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n","    cri_a = np.round(cri_a)\n","    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:09.173423Z","iopub.status.busy":"2024-07-05T04:55:09.172833Z","iopub.status.idle":"2024-07-05T04:55:09.180074Z","shell.execute_reply":"2024-07-05T04:55:09.179064Z","shell.execute_reply.started":"2024-07-05T04:55:09.173392Z"},"trusted":true},"outputs":[],"source":["def importstr(module_str, from_=None):\n","    \"\"\"\n","    >>> importstr('os')\n","    <module 'os' from '.../os.pyc'>\n","    >>> importstr('math', 'fabs')\n","    <built-in function fabs>\n","    \"\"\"\n","    if from_ is None and ':' in module_str:\n","        module_str, from_ = module_str.rsplit(':')\n","\n","    module = __import__(module_str)\n","    for sub_str in module_str.split('.')[1:]:\n","        module = getattr(module, sub_str)\n","\n","    if from_:\n","        try:\n","            return getattr(module, from_)\n","        except:\n","            raise ImportError('{}.{}'.format(module_str, from_))\n","    return module"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:09.740913Z","iopub.status.busy":"2024-07-05T04:55:09.740076Z","iopub.status.idle":"2024-07-05T04:55:09.746902Z","shell.execute_reply":"2024-07-05T04:55:09.745997Z","shell.execute_reply.started":"2024-07-05T04:55:09.740879Z"},"trusted":true},"outputs":[],"source":["import logging\n","import logging.handlers\n","\n","root_logger = logging.getLogger()\n","root_logger.setLevel(logging.INFO)\n","\n","# Some libraries attempt to add their own root logger handlers. This is\n","# annoying and so we get rid of them.\n","for handler in list(root_logger.handlers):\n","    root_logger.removeHandler(handler)\n","\n","logfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\n","formatter = logging.Formatter(logfmt_str)\n","\n","streamHandler = logging.StreamHandler()\n","streamHandler.setFormatter(formatter)\n","streamHandler.setLevel(logging.DEBUG)\n","\n","root_logger.addHandler(streamHandler)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:10.245080Z","iopub.status.busy":"2024-07-05T04:55:10.244454Z","iopub.status.idle":"2024-07-05T04:55:10.249413Z","shell.execute_reply":"2024-07-05T04:55:10.248386Z","shell.execute_reply.started":"2024-07-05T04:55:10.245051Z"},"trusted":true},"outputs":[],"source":["log = logging.getLogger('nb')\n","log.setLevel(logging.DEBUG)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:10.864802Z","iopub.status.busy":"2024-07-05T04:55:10.864450Z","iopub.status.idle":"2024-07-05T04:55:10.877625Z","shell.execute_reply":"2024-07-05T04:55:10.876680Z","shell.execute_reply.started":"2024-07-05T04:55:10.864774Z"},"trusted":true},"outputs":[],"source":["def enumerateWithEstimate(\n","        iter,\n","        desc_str,\n","        start_ndx=0,\n","        print_ndx=4,\n","        backoff=None,\n","        iter_len=None,\n","):\n","    \"\"\"\n","    In terms of behavior, `enumerateWithEstimate` is almost identical\n","    to the standard `enumerate` (the differences are things like how\n","    our function returns a generator, while `enumerate` returns a\n","    specialized `<enumerate object at 0x...>`).\n","\n","    However, the side effects (logging, specifically) are what make the\n","    function interesting.\n","\n","    :param iter: `iter` is the iterable that will be passed into\n","        `enumerate`. Required.\n","\n","    :param desc_str: This is a human-readable string that describes\n","        what the loop is doing. The value is arbitrary, but should be\n","        kept reasonably short. Things like `\"epoch 4 training\"` or\n","        `\"deleting temp files\"` or similar would all make sense.\n","\n","    :param start_ndx: This parameter defines how many iterations of the\n","        loop should be skipped before timing actually starts. Skipping\n","        a few iterations can be useful if there are startup costs like\n","        caching that are only paid early on, resulting in a skewed\n","        average when those early iterations dominate the average time\n","        per iteration.\n","\n","        NOTE: Using `start_ndx` to skip some iterations makes the time\n","        spent performing those iterations not be included in the\n","        displayed duration. Please account for this if you use the\n","        displayed duration for anything formal.\n","\n","        This parameter defaults to `0`.\n","\n","    :param print_ndx: determines which loop interation that the timing\n","        logging will start on. The intent is that we don't start\n","        logging until we've given the loop a few iterations to let the\n","        average time-per-iteration a chance to stablize a bit. We\n","        require that `print_ndx` not be less than `start_ndx` times\n","        `backoff`, since `start_ndx` greater than `0` implies that the\n","        early N iterations are unstable from a timing perspective.\n","\n","        `print_ndx` defaults to `4`.\n","\n","    :param backoff: This is used to how many iterations to skip before\n","        logging again. Frequent logging is less interesting later on,\n","        so by default we double the gap between logging messages each\n","        time after the first.\n","\n","        `backoff` defaults to `2` unless iter_len is > 1000, in which\n","        case it defaults to `4`.\n","\n","    :param iter_len: Since we need to know the number of items to\n","        estimate when the loop will finish, that can be provided by\n","        passing in a value for `iter_len`. If a value isn't provided,\n","        then it will be set by using the value of `len(iter)`.\n","\n","    :return:\n","    \"\"\"\n","    if iter_len is None:\n","        iter_len = len(iter)\n","\n","    if backoff is None:\n","        backoff = 2\n","        while backoff ** 7 < iter_len:\n","            backoff *= 2\n","\n","    assert backoff >= 2\n","    while print_ndx < start_ndx * backoff:\n","        print_ndx *= backoff\n","\n","    log.warning(\"{} ----/{}, starting\".format(\n","        desc_str,\n","        iter_len,\n","    ))\n","    start_ts = time.time()\n","    for (current_ndx, item) in enumerate(iter):\n","        yield (current_ndx, item)\n","        if current_ndx == print_ndx:\n","            # ... <1>\n","            duration_sec = ((time.time() - start_ts)\n","                            / (current_ndx - start_ndx + 1)\n","                            * (iter_len-start_ndx)\n","                            )\n","\n","            done_dt = datetime.datetime.fromtimestamp(start_ts + duration_sec)\n","            done_td = datetime.timedelta(seconds=duration_sec)\n","\n","            log.info(\"{} {:-4}/{}, done at {}, {}\".format(\n","                desc_str,\n","                current_ndx,\n","                iter_len,\n","                str(done_dt).rsplit('.', 1)[0],\n","                str(done_td).rsplit('.', 1)[0],\n","            ))\n","\n","            print_ndx *= backoff\n","\n","        if current_ndx + 1 == start_ndx:\n","            start_ts = time.time()\n","\n","    log.warning(\"{} ----/{}, done at {}\".format(\n","        desc_str,\n","        iter_len,\n","        str(datetime.datetime.now()).rsplit('.', 1)[0],\n","    ))"]},{"cell_type":"markdown","metadata":{},"source":["# Disk"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:11.948810Z","iopub.status.busy":"2024-07-05T04:55:11.948098Z","iopub.status.idle":"2024-07-05T04:55:11.963479Z","shell.execute_reply":"2024-07-05T04:55:11.962572Z","shell.execute_reply.started":"2024-07-05T04:55:11.948780Z"},"trusted":true},"outputs":[],"source":["import gzip\n","\n","from diskcache import FanoutCache, Disk\n","from diskcache.core import MODE_BINARY # delete BytesType and BytesIO declarations\n","\n","BytesType = bytes # Import them by ourselves\n","import io\n","BytesIO = io.BytesIO\n","\n","log = logging.getLogger(__name__)\n","# log.setLevel(logging.WARN)\n","log.setLevel(logging.INFO)\n","# log.setLevel(logging.DEBUG)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:12.491788Z","iopub.status.busy":"2024-07-05T04:55:12.490945Z","iopub.status.idle":"2024-07-05T04:55:12.503002Z","shell.execute_reply":"2024-07-05T04:55:12.502113Z","shell.execute_reply.started":"2024-07-05T04:55:12.491756Z"},"trusted":true},"outputs":[],"source":["class GzipDisk(Disk):\n","    def store(self, value, read, key=None):\n","        \"\"\"\n","        Override from base class diskcache.Disk.\n","\n","        Chunking is due to needing to work on pythons < 2.7.13:\n","        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n","          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n","          compression and decompression operations did not properly handle results of\n","          2 or 4 GiB.\n","\n","        :param value: value to convert\n","        :param bool read: True when value is file-like object\n","        :return: (size, mode, filename, value) tuple for Cache table\n","        \"\"\"\n","        # pylint: disable=unidiomatic-typecheck\n","        if type(value) is BytesType:\n","            if read:\n","                value = value.read()\n","                read = False\n","\n","            str_io = BytesIO()\n","            gz_file = gzip.GzipFile(mode='wb', compresslevel=1, fileobj=str_io)\n","\n","            for offset in range(0, len(value), 2**30):\n","                gz_file.write(value[offset:offset+2**30])\n","            gz_file.close()\n","\n","            value = str_io.getvalue()\n","\n","        return super(GzipDisk, self).store(value, read)\n","\n","\n","    def fetch(self, mode, filename, value, read):\n","        \"\"\"\n","        Override from base class diskcache.Disk.\n","\n","        Chunking is due to needing to work on pythons < 2.7.13:\n","        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n","          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n","          compression and decompression operations did not properly handle results of\n","          2 or 4 GiB.\n","\n","        :param int mode: value mode raw, binary, text, or pickle\n","        :param str filename: filename of corresponding value\n","        :param value: database value\n","        :param bool read: when True, return an open file handle\n","        :return: corresponding Python value\n","        \"\"\"\n","        value = super(GzipDisk, self).fetch(mode, filename, value, read)\n","\n","        if mode == MODE_BINARY:\n","            str_io = BytesIO(value)\n","            gz_file = gzip.GzipFile(mode='rb', fileobj=str_io)\n","            read_csio = BytesIO()\n","\n","            while True:\n","                uncompressed_data = gz_file.read(2**30)\n","                if uncompressed_data:\n","                    read_csio.write(uncompressed_data)\n","                else:\n","                    break\n","\n","            value = read_csio.getvalue()\n","\n","        return value\n","\n","def getCache(scope_str):\n","    return FanoutCache('cache/' + scope_str,\n","                       disk=GzipDisk,\n","                       shards=64,\n","                       timeout=1,\n","                       size_limit=3e11,\n","                       # disk_min_file_size=2**20,\n","                       )"]},{"cell_type":"markdown","metadata":{},"source":["# Visialize"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:13.621883Z","iopub.status.busy":"2024-07-05T04:55:13.621541Z","iopub.status.idle":"2024-07-05T04:55:13.631418Z","shell.execute_reply":"2024-07-05T04:55:13.630577Z","shell.execute_reply.started":"2024-07-05T04:55:13.621856Z"},"trusted":true},"outputs":[],"source":["import matplotlib\n","matplotlib.use('nbagg')\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:14.221752Z","iopub.status.busy":"2024-07-05T04:55:14.221365Z","iopub.status.idle":"2024-07-05T04:55:14.246680Z","shell.execute_reply":"2024-07-05T04:55:14.245690Z","shell.execute_reply.started":"2024-07-05T04:55:14.221726Z"},"trusted":true},"outputs":[],"source":["clim=(-1000.0, 300)\n","\n","def findPositiveSamples(start_ndx=0, limit=100):\n","    ds = LunaDataset(sortby_str='label_and_size')\n","\n","    positiveSample_list = []\n","    for sample_tup in ds.candidateInfo_list:\n","        if sample_tup.isNodule_bool:\n","            print(len(positiveSample_list), sample_tup)\n","            positiveSample_list.append(sample_tup)\n","\n","        if len(positiveSample_list) >= limit:\n","            break\n","\n","    return positiveSample_list\n","\n","def showCandidate(series_uid, batch_ndx=None, **kwargs):\n","    ds = LunaDataset(series_uid=series_uid, **kwargs)\n","    pos_list = [i for i, x in enumerate(ds.candidateInfo_list) if x.isNodule_bool]\n","\n","    if batch_ndx is None:\n","        if pos_list:\n","            batch_ndx = pos_list[0]\n","        else:\n","            print(\"Warning: no positive samples found; using first negative sample.\")\n","            batch_ndx = 0\n","\n","    ct = Ct(series_uid)\n","    ct_t, pos_t, series_uid, center_irc = ds[batch_ndx]\n","    ct_a = ct_t[0].numpy()\n","\n","    fig = plt.figure(figsize=(30, 50))\n","\n","    group_list = [\n","        [9, 11, 13],\n","        [15, 16, 17],\n","        [19, 21, 23],\n","    ]\n","\n","    subplot = fig.add_subplot(len(group_list) + 2, 3, 1)\n","    subplot.set_title('index {}'.format(int(center_irc.index)), fontsize=30)\n","    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n","        label.set_fontsize(20)\n","    plt.imshow(ct.hu_a[int(center_irc.index)], clim=clim, cmap='gray')\n","\n","    subplot = fig.add_subplot(len(group_list) + 2, 3, 2)\n","    subplot.set_title('row {}'.format(int(center_irc.row)), fontsize=30)\n","    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n","        label.set_fontsize(20)\n","    plt.imshow(ct.hu_a[:,int(center_irc.row)], clim=clim, cmap='gray')\n","    plt.gca().invert_yaxis()\n","\n","    subplot = fig.add_subplot(len(group_list) + 2, 3, 3)\n","    subplot.set_title('col {}'.format(int(center_irc.col)), fontsize=30)\n","    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n","        label.set_fontsize(20)\n","    plt.imshow(ct.hu_a[:,:,int(center_irc.col)], clim=clim, cmap='gray')\n","    plt.gca().invert_yaxis()\n","\n","    subplot = fig.add_subplot(len(group_list) + 2, 3, 4)\n","    subplot.set_title('index {}'.format(int(center_irc.index)), fontsize=30)\n","    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n","        label.set_fontsize(20)\n","    plt.imshow(ct_a[ct_a.shape[0]//2], clim=clim, cmap='gray')\n","\n","    subplot = fig.add_subplot(len(group_list) + 2, 3, 5)\n","    subplot.set_title('row {}'.format(int(center_irc.row)), fontsize=30)\n","    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n","        label.set_fontsize(20)\n","    plt.imshow(ct_a[:,ct_a.shape[1]//2], clim=clim, cmap='gray')\n","    plt.gca().invert_yaxis()\n","\n","    subplot = fig.add_subplot(len(group_list) + 2, 3, 6)\n","    subplot.set_title('col {}'.format(int(center_irc.col)), fontsize=30)\n","    for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n","        label.set_fontsize(20)\n","    plt.imshow(ct_a[:,:,ct_a.shape[2]//2], clim=clim, cmap='gray')\n","    plt.gca().invert_yaxis()\n","\n","    for row, index_list in enumerate(group_list):\n","        for col, index in enumerate(index_list):\n","            subplot = fig.add_subplot(len(group_list) + 2, 3, row * 3 + col + 7)\n","            subplot.set_title('slice {}'.format(index), fontsize=30)\n","            for label in (subplot.get_xticklabels() + subplot.get_yticklabels()):\n","                label.set_fontsize(20)\n","            plt.imshow(ct_a[index], clim=clim, cmap='gray')\n","\n","\n","    print(series_uid, batch_ndx, bool(pos_t[0]), pos_list)\n","\n","\n","def build2dLungMask(series_uid, center_ndx):\n","    mask_model = SegmentationMask().to('cuda')\n","    ct = Ct(series_uid)\n","\n","    ct_g = torch.from_numpy(ct.hu_a[center_ndx].astype(np.float32)).unsqueeze(0).unsqueeze(0).to('cuda')\n","    pos_g = torch.from_numpy(ct.positive_mask[center_ndx].astype(np.float32)).unsqueeze(0).unsqueeze(0).to('cuda')\n","    input_g = ct_g / 1000\n","\n","    label_g, neg_g, pos_g, lung_mask, mask_dict = mask_model(input_g, pos_g)\n","    mask_tup = MaskTuple(**mask_dict)\n","\n","    return mask_tup"]},{"cell_type":"markdown","metadata":{},"source":["# Datasets"]},{"cell_type":"markdown","metadata":{},"source":["### Import Libraries"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:16.070175Z","iopub.status.busy":"2024-07-05T04:55:16.069309Z","iopub.status.idle":"2024-07-05T04:55:21.346093Z","shell.execute_reply":"2024-07-05T04:55:21.345263Z","shell.execute_reply.started":"2024-07-05T04:55:16.070142Z"},"trusted":true},"outputs":[],"source":["import copy\n","import csv\n","import functools\n","import glob\n","import os\n","\n","from collections import namedtuple\n","\n","import SimpleITK as sitk\n","import numpy as np\n","\n","import torch\n","import torch.cuda\n","from torch.utils.data import Dataset\n","\n","raw_cache = getCache('seg_raw')\n","\n","# log = logging.getLogger(__name__)\n","# # log.setLevel(logging.WARN)\n","# # log.setLevel(logging.INFO)\n","# log.setLevel(logging.DEBUG)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:21.347847Z","iopub.status.busy":"2024-07-05T04:55:21.347468Z","iopub.status.idle":"2024-07-05T04:55:21.352893Z","shell.execute_reply":"2024-07-05T04:55:21.351904Z","shell.execute_reply.started":"2024-07-05T04:55:21.347821Z"},"trusted":true},"outputs":[],"source":["MaskTuple = namedtuple(\n","    'MaskTuple', \n","    'raw_dense_mask, dense_mask, body_mask, air_mask, raw_candidate_mask, candidate_mask, lung_mask, neg_mask, pos_mask'\n",")\n","\n","CandidateInfoTuple = namedtuple(\n","    'CandidateInfoTuple',\n","    'isNodule_bool, hasAnnotation_bool, isMal_bool, diameter_mm, series_uid, center_xyz',\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:22.997758Z","iopub.status.busy":"2024-07-05T04:55:22.997401Z","iopub.status.idle":"2024-07-05T04:55:23.009152Z","shell.execute_reply":"2024-07-05T04:55:23.008252Z","shell.execute_reply.started":"2024-07-05T04:55:22.997730Z"},"trusted":true},"outputs":[],"source":["LIST_LIMIT = 50000\n","@functools.lru_cache(1)\n","def getCandidateInfoList(requireOnDisk_bool=True):\n","    mhd_list = glob.glob('/kaggle/input/luna16/subset[0-5]/subset*/*.mhd')\n","    presentOnDisk_set= {os.path.split(p)[-1][:-4] for p in mhd_list}\n","    \n","    candidateInfo_list = []\n","    with open('/kaggle/input/annotations/annotations_with_malignancy.csv', \"r\") as f:\n","        for row in list(csv.reader(f))[1:]: # skip title row\n","            series_uid = row[0]\n","            \n","            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n","                continue\n","            \n","            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n","            annotationDiameter_mm = float(row[4])\n","            isMal_bool = {'False': False, 'True': True}[row[5]]\n","            \n","            candidateInfo_list.append(\n","                CandidateInfoTuple(\n","                    True,\n","                    True,\n","                    isMal_bool,\n","                    annotationDiameter_mm,\n","                    series_uid,\n","                    annotationCenter_xyz,\n","                )\n","            )\n","    \n","#     print(len(candidateInfo_list))\n","    \n","    with open('/kaggle/input/luna16/candidates.csv', 'r') as f:\n","        for row in list(csv.reader(f))[1:]:\n","            series_uid = row[0]\n","            \n","            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n","                continue\n","            \n","            isNodule_bool = bool(int(row[4]))\n","            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n","            \n","            if not isNodule_bool:\n","                candidateInfo_list.append(\n","                    CandidateInfoTuple(\n","                        False,\n","                        False,\n","                        False,\n","                        0.0,\n","                        series_uid,\n","                        candidateCenter_xyz,\n","                    )\n","                )\n","            \n","            if len(candidateInfo_list) >= LIST_LIMIT:\n","                break\n","    \n","#     print(len(candidateInfo_list))\n","            \n","    candidateInfo_list.sort(reverse=True)\n","    return candidateInfo_list"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:29.228052Z","iopub.status.busy":"2024-07-05T04:55:29.227213Z","iopub.status.idle":"2024-07-05T04:55:29.234077Z","shell.execute_reply":"2024-07-05T04:55:29.233063Z","shell.execute_reply.started":"2024-07-05T04:55:29.228012Z"},"trusted":true},"outputs":[],"source":["@functools.lru_cache(1)\n","def getCandidateInfoDict(requireOnDisk_bool=True):\n","    candidateInfo_list = getCandidateInfoList(requireOnDisk_bool)\n","    candidateInfo_dict = {}\n","\n","    for candidateInfo_tup in candidateInfo_list:\n","        candidateInfo_dict.setdefault(candidateInfo_tup.series_uid,\n","                                      []).append(candidateInfo_tup)\n","\n","    return candidateInfo_dict"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:30.363984Z","iopub.status.busy":"2024-07-05T04:55:30.363308Z","iopub.status.idle":"2024-07-05T04:55:30.383877Z","shell.execute_reply":"2024-07-05T04:55:30.382986Z","shell.execute_reply.started":"2024-07-05T04:55:30.363944Z"},"trusted":true},"outputs":[],"source":["class Ct:\n","    def __init__(self, series_uid):\n","        mhd_path = glob.glob('/kaggle/input/luna16/subset[0-5]/subset*/{}.mhd'.format(series_uid))[0]\n","        ct_mhd = sitk.ReadImage(mhd_path)\n","        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n","        \n","        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n","        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n","        \n","        self.series_uid = series_uid\n","        self.hu_a = ct_a\n","        \n","        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n","        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n","        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n","        \n","        candidateInfo_list = getCandidateInfoDict()[self.series_uid]\n","        \n","        self.positiveInfo_list = [\n","            candidate_tup for candidate_tup in candidateInfo_list if candidate_tup.isNodule_bool\n","        ]\n","        \n","        self.positive_Mask = self.buildAnnotationMask(self.positiveInfo_list)\n","        self.positive_indexes = (self.positive_Mask.sum(axis=(1,2)).nonzero()[0].tolist())\n","        \n","        \n","    def buildAnnotationMask(self, positiveInfo_list, threshold_hu = -700):\n","        boundingBox_a = np.zeros_like(self.hu_a, dtype=np.bool_)\n","        \n","        for candidateInfo_tup in positiveInfo_list:\n","            center_irc = xyz2irc(candidateInfo_tup.center_xyz,\n","                                 self.origin_xyz,\n","                                 self.vxSize_xyz,\n","                                 self.direction_a)\n","            ci = int(center_irc.index)\n","            cr = int(center_irc.row)\n","            cc = int(center_irc.col)\n","            \n","            index_radius = 2\n","            try:\n","                while self.hu_a[ci+index_radius, cr, cc] > threshold_hu and \\\n","                        self.hu_a[ci-index_radius, cr, cc] > threshold_hu:\n","                    index_radius += 1\n","            except IndexError:\n","                index_radius -= 1\n","                \n","            row_radius = 2\n","            try:\n","                while self.hu_a[ci, cr + row_radius, cc] > threshold_hu and \\\n","                        self.hu_a[ci, cr - row_radius, cc] > threshold_hu:\n","                    row_radius += 1\n","            except IndexError:\n","                row_radius -= 1\n","                \n","            col_radius = 2\n","            try:\n","                while self.hu_a[ci, cr, cc + col_radius] > threshold_hu and \\\n","                        self.hu_a[ci, cr, cc - col_radius] > threshold_hu:\n","                    col_radius += 1\n","            except IndexError:\n","                col_radius -= 1\n","                \n","            boundingBox_a[\n","                ci - index_radius:ci + index_radius + 1,\n","                cr - row_radius:cr + row_radius + 1,\n","                cc - col_radius:cc + col_radius + 1] = True\n","            \n","        mask_a = boundingBox_a & (self.hu_a > threshold_hu)\n","        \n","        return mask_a\n","        \n","        \n","    def getRawCandidate(self, center_xyz, width_irc):\n","        center_irc = xyz2irc(\n","            center_xyz, \n","            self.origin_xyz,\n","            self.vxSize_xyz,\n","            self.direction_a,\n","        )\n","        \n","        slice_list = []\n","        for axis, center_val in enumerate(center_irc):\n","            start_ndx = int(round(center_val - width_irc[axis]/2))\n","            end_ndx = int(start_ndx + width_irc[axis])\n","            \n","            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n","            \n","            if start_ndx < 0:\n","                start_ndx = 0\n","                end_ndx = int(width_irc[axis])\n","            \n","            if end_ndx > self.hu_a.shape[axis]:\n","                end_ndx = self.hu_a.shape[axis]\n","                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n","            \n","            slice_list.append(slice(start_ndx, end_ndx))\n","            \n","        ct_chunk = self.hu_a[tuple(slice_list)]\n","        pos_chunk = self.positive_Mask[tuple(slice_list)]\n","        \n","        return ct_chunk, pos_chunk, center_irc"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:33.196694Z","iopub.status.busy":"2024-07-05T04:55:33.196275Z","iopub.status.idle":"2024-07-05T04:55:33.203843Z","shell.execute_reply":"2024-07-05T04:55:33.202848Z","shell.execute_reply.started":"2024-07-05T04:55:33.196658Z"},"trusted":true},"outputs":[],"source":["@functools.lru_cache(1, typed=True)\n","def getCt(series_id):\n","    return Ct(series_id)\n","\n","@raw_cache.memoize(typed=True)\n","def getCtRawCandidate(series_id, center_xyz, width_irc):\n","    ct = getCt(series_id)\n","    ct_chunck, pos_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n","    ct_chunck.clip(-1000, 1000, ct_chunck)\n","    return ct_chunck, pos_chunk, center_irc\n","\n","@raw_cache.memoize(typed=True)\n","def getCtSampleSize(series_uid):\n","    ct = Ct(series_uid)\n","    return int(ct.hu_a.shape[0]), ct.positive_indexes"]},{"cell_type":"markdown","metadata":{},"source":["### Create Dataset"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:35.053757Z","iopub.status.busy":"2024-07-05T04:55:35.052901Z","iopub.status.idle":"2024-07-05T04:55:35.068566Z","shell.execute_reply":"2024-07-05T04:55:35.067617Z","shell.execute_reply.started":"2024-07-05T04:55:35.053724Z"},"trusted":true},"outputs":[],"source":["class Luna2dSegmentationDataset(Dataset):\n","    def __init__(self, val_stride=10, isValSet_bool=None, series_uid=None,\n","                 contextSlices_count=3, fullCt_bool=False):\n","        self.contextSlices_count = contextSlices_count\n","        self.fullCt_bool = fullCt_bool\n","        \n","        if series_uid:\n","            self.series_list = [series_uid]\n","        else:\n","            self.series_list = sorted(getCandidateInfoDict().keys())\n","        \n","        if isValSet_bool:\n","            assert val_stride > 0, val_stride\n","            self.series_list = self.series_list[::val_stride]\n","            assert self.series_list\n","        elif val_stride > 0:\n","            del self.series_list[::val_stride]\n","            assert self.series_list\n","        \n","        self.sample_list = []\n","        for series_uid in self.series_list:\n","            index_count, positive_indexes = getCtSampleSize(series_uid)\n","            \n","            if self.fullCt_bool:\n","                self.sample_list += [(series_uid, slice_ndx) for slice_ndx in range(index_count)]\n","            else:\n","                self.sample_list += [(series_uid, slice_ndx) for slice_ndx in positive_indexes]\n","            \n","        self.candidateInfo_list = getCandidateInfoList()\n","        \n","        series_set = set(self.series_list)\n","        self.candidateInfo_list = [cit for cit in self.candidateInfo_list \n","                                   if cit.series_uid in series_set]\n","        \n","        self.pos_list = [nt for nt in self.candidateInfo_list \n","                         if nt.isNodule_bool]\n","        \n","        log.info(\"{!r}: {} {} series, {} slices, {} nodules\".format(\n","            self,\n","            len(self.series_list),\n","            {None: 'general', True: 'validation', False: 'training'}[isValSet_bool],\n","            len(self.sample_list),\n","            len(self.pos_list),\n","        ))\n","        \n","    def __len__(self):\n","        return len(self.sample_list)\n","    \n","    def __getitem__(self, ndx):\n","        series_uid, slice_ndx = self.sample_list[ndx % len(self.sample_list)]\n","        return self.getitem_fullSlice(series_uid, slice_ndx)\n","            \n","    def getitem_fullSlice(self, series_uid, slice_ndx):\n","        ct = getCt(series_uid)\n","        ct_t = torch.zeros((self.contextSlices_count * 2 + 1, 512, 512))\n","        \n","        start_ndx = slice_ndx - self.contextSlices_count\n","        end_ndx = slice_ndx + self.contextSlices_count + 1\n","        for i, context_ndx in enumerate(range(start_ndx, end_ndx)):\n","            context_ndx = max(context_ndx, 0)\n","            context_ndx = min(context_ndx, ct.hu_a.shape[0] - 1)\n","            ct_t[i] = torch.from_numpy(ct.hu_a[context_ndx].astype(np.float32))\n","        \n","        ct_t.clamp_(-1000, 1000)\n","        \n","        pos_t = torch.from_numpy(ct.positive_Mask[slice_ndx]).unsqueeze(0)\n","        \n","        return ct_t, pos_t, ct.series_uid, slice_ndx\n","        "]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:36.189515Z","iopub.status.busy":"2024-07-05T04:55:36.189150Z","iopub.status.idle":"2024-07-05T04:55:36.199188Z","shell.execute_reply":"2024-07-05T04:55:36.198123Z","shell.execute_reply.started":"2024-07-05T04:55:36.189486Z"},"trusted":true},"outputs":[],"source":["class TrainingLuna2dSegmentationDataset(Luna2dSegmentationDataset):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        \n","        self.ratio_int = 2\n","    \n","    def __len__(self):\n","        return 10000\n","    \n","    def shuffleSamples(self):\n","        random.shuffle(self.candidateInfo_list)\n","        random.shuffle(self.pos_list)\n","        \n","    def __getitem__(self, ndx):\n","        candidateInfo_tup = self.pos_list[ndx % len(self.pos_list)]\n","        return self.getitem_trainingCrop(candidateInfo_tup)\n","    \n","    def getitem_trainingCrop(self, candidateInfo_tup):\n","        ct_a, pos_a, center_irc = getCtRawCandidate(\n","            candidateInfo_tup.series_uid,\n","            candidateInfo_tup.center_xyz,\n","            (7, 96, 96)\n","        )\n","        pos_a = pos_a[3:4]\n","        \n","        row_offset = random.randrange(0, 32)\n","        col_offset = random.randrange(0, 32)\n","        ct_t = torch.from_numpy(ct_a[:, row_offset:row_offset+64, col_offset:col_offset+64]).to(torch.float32)\n","        pos_t = torch.from_numpy(pos_a[:, row_offset:row_offset+64, col_offset:col_offset+64]).to(torch.long)\n","        \n","        slice_ndx = center_irc.index\n","        \n","        return ct_t, pos_t, candidateInfo_tup.series_uid, slice_ndx\n","        \n","        "]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:55:37.671086Z","iopub.status.busy":"2024-07-05T04:55:37.670355Z","iopub.status.idle":"2024-07-05T04:55:37.678838Z","shell.execute_reply":"2024-07-05T04:55:37.677884Z","shell.execute_reply.started":"2024-07-05T04:55:37.671054Z"},"trusted":true},"outputs":[],"source":["class PrepcacheLunaDataset(Dataset):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","\n","        self.candidateInfo_list = getCandidateInfoList()\n","        self.pos_list = [nt for nt in self.candidateInfo_list if nt.isNodule_bool]\n","\n","        self.seen_set = set()\n","        self.candidateInfo_list.sort(key=lambda x: x.series_uid)\n","\n","    def __len__(self):\n","        return len(self.candidateInfo_list)\n","\n","    def __getitem__(self, ndx):\n","        # candidate_t, pos_t, series_uid, center_t = super().__getitem__(ndx)\n","\n","        candidateInfo_tup = self.candidateInfo_list[ndx]\n","        getCtRawCandidate(candidateInfo_tup.series_uid, candidateInfo_tup.center_xyz, (7, 96, 96))\n","\n","        series_uid = candidateInfo_tup.series_uid\n","        if series_uid not in self.seen_set:\n","            self.seen_set.add(series_uid)\n","\n","            getCtSampleSize(series_uid)\n","            # ct = getCt(series_uid)\n","            # for mask_ndx in ct.positive_indexes:\n","            #     build2dLungMask(series_uid, mask_ndx)\n","\n","        return 0, 1 #candidate_t, pos_t, series_uid, center_t"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:56:32.502119Z","iopub.status.busy":"2024-07-05T04:56:32.501360Z","iopub.status.idle":"2024-07-05T04:56:32.506427Z","shell.execute_reply":"2024-07-05T04:56:32.505461Z","shell.execute_reply.started":"2024-07-05T04:56:32.502087Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.optim import SGD, Adam\n","from torch.utils.data import DataLoader"]},{"cell_type":"markdown","metadata":{},"source":["#### U-Net for Segmentation"]},{"cell_type":"code","execution_count":25,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-07-05T04:56:36.157421Z","iopub.status.busy":"2024-07-05T04:56:36.157038Z","iopub.status.idle":"2024-07-05T04:56:36.182832Z","shell.execute_reply":"2024-07-05T04:56:36.181801Z","shell.execute_reply.started":"2024-07-05T04:56:36.157391Z"},"trusted":true},"outputs":[],"source":["# From https://github.com/jvanvugt/pytorch-unet\n","# https://raw.githubusercontent.com/jvanvugt/pytorch-unet/master/unet.py\n","\n","# MIT License\n","#\n","# Copyright (c) 2018 Joris\n","#\n","# Permission is hereby granted, free of charge, to any person obtaining a copy\n","# of this software and associated documentation files (the \"Software\"), to deal\n","# in the Software without restriction, including without limitation the rights\n","# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","# copies of the Software, and to permit persons to whom the Software is\n","# furnished to do so, subject to the following conditions:\n","#\n","# The above copyright notice and this permission notice shall be included in all\n","# copies or substantial portions of the Software.\n","#\n","# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","# SOFTWARE.\n","\n","# Adapted from https://discuss.pytorch.org/t/unet-implementation/426\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, in_channels=1, n_classes=2, depth=5, wf=6, padding=False,\n","                 batch_norm=False, up_mode='upconv'):\n","        \"\"\"\n","        Implementation of\n","        U-Net: Convolutional Networks for Biomedical Image Segmentation\n","        (Ronneberger et al., 2015)\n","        https://arxiv.org/abs/1505.04597\n","\n","        Using the default arguments will yield the exact version used\n","        in the original paper\n","\n","        Args:\n","            in_channels (int): number of input channels\n","            n_classes (int): number of output channels\n","            depth (int): depth of the network\n","            wf (int): number of filters in the first layer is 2**wf\n","            padding (bool): if True, apply padding such that the input shape\n","                            is the same as the output.\n","                            This may introduce artifacts\n","            batch_norm (bool): Use BatchNorm after layers with an\n","                               activation function\n","            up_mode (str): one of 'upconv' or 'upsample'.\n","                           'upconv' will use transposed convolutions for\n","                           learned upsampling.\n","                           'upsample' will use bilinear upsampling.\n","        \"\"\"\n","        super(UNet, self).__init__()\n","        assert up_mode in ('upconv', 'upsample')\n","        self.padding = padding\n","        self.depth = depth\n","        prev_channels = in_channels\n","        self.down_path = nn.ModuleList()\n","        for i in range(depth):\n","            self.down_path.append(UNetConvBlock(prev_channels, 2**(wf+i),\n","                                                padding, batch_norm))\n","            prev_channels = 2**(wf+i)\n","\n","        self.up_path = nn.ModuleList()\n","        for i in reversed(range(depth - 1)):\n","            self.up_path.append(UNetUpBlock(prev_channels, 2**(wf+i), up_mode,\n","                                            padding, batch_norm))\n","            prev_channels = 2**(wf+i)\n","\n","        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        blocks = []\n","        for i, down in enumerate(self.down_path):\n","            x = down(x)\n","            if i != len(self.down_path)-1:\n","                blocks.append(x)\n","                x = F.avg_pool2d(x, 2)\n","\n","        for i, up in enumerate(self.up_path):\n","            x = up(x, blocks[-i-1])\n","\n","        return self.last(x)\n","\n","\n","class UNetConvBlock(nn.Module):\n","    def __init__(self, in_size, out_size, padding, batch_norm):\n","        super(UNetConvBlock, self).__init__()\n","        block = []\n","\n","        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n","                               padding=int(padding)))\n","        block.append(nn.ReLU())\n","        # block.append(nn.LeakyReLU())\n","        if batch_norm:\n","            block.append(nn.BatchNorm2d(out_size))\n","\n","        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n","                               padding=int(padding)))\n","        block.append(nn.ReLU())\n","        # block.append(nn.LeakyReLU())\n","        if batch_norm:\n","            block.append(nn.BatchNorm2d(out_size))\n","\n","        self.block = nn.Sequential(*block)\n","\n","    def forward(self, x):\n","        out = self.block(x)\n","        return out\n","\n","\n","class UNetUpBlock(nn.Module):\n","    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n","        super(UNetUpBlock, self).__init__()\n","        if up_mode == 'upconv':\n","            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n","                                         stride=2)\n","        elif up_mode == 'upsample':\n","            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n","                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n","\n","        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n","\n","    def center_crop(self, layer, target_size):\n","        _, _, layer_height, layer_width = layer.size()\n","        diff_y = (layer_height - target_size[0]) // 2\n","        diff_x = (layer_width - target_size[1]) // 2\n","        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n","\n","    def forward(self, x, bridge):\n","        up = self.up(x)\n","        crop1 = self.center_crop(bridge, up.shape[2:])\n","        out = torch.cat([up, crop1], 1)\n","        out = self.conv_block(out)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:56:37.388710Z","iopub.status.busy":"2024-07-05T04:56:37.387857Z","iopub.status.idle":"2024-07-05T04:56:37.397352Z","shell.execute_reply":"2024-07-05T04:56:37.396404Z","shell.execute_reply.started":"2024-07-05T04:56:37.388677Z"},"trusted":true},"outputs":[],"source":["class UNetWrapper(nn.Module):\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","        \n","        self.input_batchnorm = nn.BatchNorm2d(kwargs['in_channels'])\n","        self.unet = UNet(**kwargs)\n","        self.final = nn.Sigmoid()\n","        \n","        self._init_weights()\n","        \n","    def _init_weights(self):\n","        init_set = {\n","            nn.Conv2d,\n","            nn.Conv3d,\n","            nn.ConvTranspose2d,\n","            nn.ConvTranspose3d,\n","            nn.Linear,\n","        }\n","        for m in self.modules():\n","            if type(m) in init_set:\n","                nn.init.kaiming_normal_(\n","                    m.weight.data, mode='fan_out', nonlinearity='relu', a=0\n","                )\n","                if m.bias is not None:\n","                    fan_in, fan_out = \\\n","                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n","                    bound = 1 / math.sqrt(fan_out)\n","                    nn.init.normal_(m.bias, -bound, bound)\n","                    \n","    def forward(self, input_batch):\n","        bn_output = self.input_batchnorm(input_batch)\n","        un_output = self.unet(bn_output)\n","        fn_output = self.final(un_output)\n","        return fn_output"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:56:37.916595Z","iopub.status.busy":"2024-07-05T04:56:37.916286Z","iopub.status.idle":"2024-07-05T04:56:37.929467Z","shell.execute_reply":"2024-07-05T04:56:37.928557Z","shell.execute_reply.started":"2024-07-05T04:56:37.916571Z"},"trusted":true},"outputs":[],"source":["class SegmentationAugmentation(nn.Module):\n","    def __init__(self, flip=None, offset=None, scale=None, rotate=None, noise=None):\n","        super().__init__()\n","        \n","        self.flip = flip\n","        self.offset = offset\n","        self.scale = scale\n","        self.rotate = rotate\n","        self.noise = noise\n","        \n","    def forward(self, input_g, label_g):\n","        transform_t = self._build2dTransformMatrix()\n","        transform_t = transform_t.expand(input_g.shape[0], -1, -1)\n","        transform_t = transform_t.to(input_g.device, torch.float32)\n","        \n","        affine_t = F.affine_grid(transform_t[:,:2],\n","                input_g.size(), align_corners=False)\n","\n","        augmented_input_g = F.grid_sample(input_g,\n","                affine_t, padding_mode='border',\n","                align_corners=False)\n","        augmented_label_g = F.grid_sample(label_g.to(torch.float32),\n","                affine_t, padding_mode='border',\n","                align_corners=False)\n","        \n","        if self.noise:\n","            noise_t = torch.randn_like(augmented_input_g)\n","            noise_t *= self.noise\n","\n","            augmented_input_g += noise_t\n","            \n","        return augmented_input_g, augmented_label_g > 0.5\n","        \n","    def _build2dTransformMatrix(self):\n","        transform_t = torch.eye(3)\n","        \n","        for i in range(2):\n","            if self.flip:\n","                if random.random() > 0.5:\n","                    transform_t[i,i] *= -1\n","\n","            if self.offset:\n","                offset_float = self.offset\n","                random_float = (random.random() * 2 - 1)\n","                transform_t[2,i] = offset_float * random_float\n","\n","            if self.scale:\n","                scale_float = self.scale\n","                random_float = (random.random() * 2 - 1)\n","                transform_t[i,i] *= 1.0 + scale_float * random_float\n","        if self.rotate:\n","            angle_rad = random.random() * math.pi * 2\n","            s = math.sin(angle_rad)\n","            c = math.cos(angle_rad)\n","\n","            rotation_t = torch.tensor([\n","                [c, -s, 0],\n","                [s, c, 0],\n","                [0, 0, 1]])\n","            \n","            transform_t @= rotation_t\n","        \n","        return transform_t"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare Catch"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:56:39.828074Z","iopub.status.busy":"2024-07-05T04:56:39.827447Z","iopub.status.idle":"2024-07-05T04:56:39.835109Z","shell.execute_reply":"2024-07-05T04:56:39.834150Z","shell.execute_reply.started":"2024-07-05T04:56:39.828043Z"},"trusted":true},"outputs":[],"source":["class LunaPrepCacheApp:\n","    @classmethod\n","    def __init__(self, sys_argv=None):\n","        if sys_argv is None:\n","            sys_argv = sys.argv[1:]\n","\n","    def main(self):\n","        log.info(\"Starting {}\".format(type(self).__name__))\n","\n","        self.prep_dl = DataLoader(\n","            PrepcacheLunaDataset(\n","                # sortby_str='series_uid',\n","            ),\n","            batch_size=1024,\n","            num_workers=4,\n","        )\n","\n","        batch_iter = enumerateWithEstimate(\n","            self.prep_dl,\n","            \"Stuffing cache\",\n","            start_ndx=4,\n","        )\n","        for batch_ndx, batch_tup in batch_iter:\n","            pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pre_catch = LunaPrepCacheApp()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-05 05:29:51,436 INFO     pid:34 __main__:022:main Starting LunaPrepCacheApp\n","2024-07-05 05:29:51,459 WARNING  pid:34 __main__:081:enumerateWithEstimate Stuffing cache ----/49, starting\n","2024-07-05 05:30:13,033 INFO     pid:34 __main__:098:enumerateWithEstimate Stuffing cache    8/49, done at 2024-07-05 05:32:02, 0:02:02\n","2024-07-05 05:30:26,248 INFO     pid:34 __main__:098:enumerateWithEstimate Stuffing cache   16/49, done at 2024-07-05 05:31:32, 0:01:33\n","2024-07-05 05:30:51,245 INFO     pid:34 __main__:098:enumerateWithEstimate Stuffing cache   32/49, done at 2024-07-05 05:31:19, 0:01:20\n","2024-07-05 05:31:09,498 WARNING  pid:34 __main__:111:enumerateWithEstimate Stuffing cache ----/49, done at 2024-07-05 05:31:09\n"]}],"source":["pre_catch.main()"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T04:56:41.581709Z","iopub.status.busy":"2024-07-05T04:56:41.581383Z","iopub.status.idle":"2024-07-05T04:56:52.848841Z","shell.execute_reply":"2024-07-05T04:56:52.847856Z","shell.execute_reply.started":"2024-07-05T04:56:41.581685Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-05 04:56:43.699934: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-05 04:56:43.700058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-05 04:56:43.817347: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-05 04:56:52,547 INFO     pid:34 numexpr.utils:161:_init_num_threads NumExpr defaulting to 4 threads.\n"]}],"source":["import argparse\n","import os\n","import shutil\n","import hashlib\n","\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# METRICS_LABEL_NDX = 0\n","METRICS_LOSS_NDX = 1\n","# METRICS_FN_LOSS_NDX = 2\n","# METRICS_ALL_LOSS_NDX = 3\n","\n","# METRICS_PTP_NDX = 4\n","# METRICS_PFN_NDX = 5\n","# METRICS_MFP_NDX = 6\n","METRICS_TP_NDX = 7\n","METRICS_FN_NDX = 8\n","METRICS_FP_NDX = 9\n","\n","METRICS_SIZE = 10"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T05:46:22.632236Z","iopub.status.busy":"2024-07-05T05:46:22.631773Z","iopub.status.idle":"2024-07-05T05:46:22.699637Z","shell.execute_reply":"2024-07-05T05:46:22.698624Z","shell.execute_reply.started":"2024-07-05T05:46:22.632195Z"},"trusted":true},"outputs":[],"source":["class SegmentationTrainingApp:\n","    def __init__(self, sys_argv=None):\n","        if sys_argv is None:\n","            sys_argv = sys.argv[1:]\n","\n","        parser = argparse.ArgumentParser()\n","        parser.add_argument('--batch-size',\n","            help='Batch size to use for training',\n","            default=16,\n","            type=int,\n","        )\n","        parser.add_argument('--num-workers',\n","            help='Number of worker processes for background data loading',\n","            default=4,\n","            type=int,\n","        )\n","        parser.add_argument('--epochs',\n","            help='Number of epochs to train for',\n","            default=1,\n","            type=int,\n","        )\n","\n","        parser.add_argument('--augmented',\n","            help=\"Augment the training data.\",\n","            action='store_true',\n","            default=False,\n","        )\n","        parser.add_argument('--augment-flip',\n","            help=\"Augment the training data by randomly flipping the data left-right, up-down, and front-back.\",\n","            action='store_true',\n","            default=False,\n","        )\n","        parser.add_argument('--augment-offset',\n","            help=\"Augment the training data by randomly offsetting the data slightly along the X and Y axes.\",\n","            action='store_true',\n","            default=False,\n","        )\n","        parser.add_argument('--augment-scale',\n","            help=\"Augment the training data by randomly increasing or decreasing the size of the candidate.\",\n","            action='store_true',\n","            default=False,\n","        )\n","        parser.add_argument('--augment-rotate',\n","            help=\"Augment the training data by randomly rotating the data around the head-foot axis.\",\n","            action='store_true',\n","            default=False,\n","        )\n","        parser.add_argument('--augment-noise',\n","            help=\"Augment the training data by randomly adding noise to the data.\",\n","            action='store_true',\n","            default=False,\n","        )\n","\n","        parser.add_argument('--tb-prefix',\n","            default='seg',\n","            help=\"Data prefix to use for Tensorboard run. Defaults to chapter.\",\n","        )\n","\n","        parser.add_argument('comment',\n","            help=\"Comment suffix for Tensorboard run.\",\n","            nargs='?',\n","            default='none',\n","        )\n","\n","        self.cli_args = parser.parse_args(sys_argv)\n","        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n","        self.totalTrainingSamples_count = 0\n","        self.trn_writer = None\n","        self.val_writer = None\n","\n","        self.augmentation_dict = {}\n","        if self.cli_args.augmented or self.cli_args.augment_flip:\n","            self.augmentation_dict['flip'] = True\n","        if self.cli_args.augmented or self.cli_args.augment_offset:\n","            self.augmentation_dict['offset'] = 0.03\n","        if self.cli_args.augmented or self.cli_args.augment_scale:\n","            self.augmentation_dict['scale'] = 0.2\n","        if self.cli_args.augmented or self.cli_args.augment_rotate:\n","            self.augmentation_dict['rotate'] = True\n","        if self.cli_args.augmented or self.cli_args.augment_noise:\n","            self.augmentation_dict['noise'] = 25.0\n","\n","        self.use_cuda = torch.cuda.is_available()\n","        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n","\n","        self.segmentation_model, self.augmentation_model = self.initModel()\n","        self.optimizer = self.initOptimizer()\n","        \n","    def initModel(self):\n","        segmentation_model = UNetWrapper(in_channels=7, n_classes=1, depth=3, wf=4, \n","                                         padding=True, batch_norm=True, up_mode='upconv')\n","        \n","        augmentation_model = SegmentationAugmentation(**self.augmentation_dict)\n","        \n","        if self.use_cuda:\n","            log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n","            if torch.cuda.device_count() > 1:\n","                segmentation_model = nn.DataParallel(segmentation_model)\n","                augmentation_model = nn.DataParallel(augmentation_model)\n","            segmentation_model = segmentation_model.to(self.device)\n","            augmentation_model = augmentation_model.to(self.device)\n","\n","        return segmentation_model, augmentation_model\n","    \n","    def initOptimizer(self):\n","        return Adam(self.segmentation_model.parameters())\n","    \n","    def initTrainDl(self):\n","        train_ds = TrainingLuna2dSegmentationDataset(\n","            val_stride=10,\n","            isValSet_bool=False,\n","            contextSlices_count=3,\n","        )\n","\n","        batch_size = self.cli_args.batch_size\n","        if self.use_cuda:\n","            batch_size *= torch.cuda.device_count()\n","\n","        train_dl = DataLoader(\n","            train_ds,\n","            batch_size=batch_size,\n","            num_workers=self.cli_args.num_workers,\n","            pin_memory=self.use_cuda,\n","        )\n","    \n","        return train_dl\n","    \n","    def initValDl(self):\n","        val_ds = Luna2dSegmentationDataset(\n","            val_stride=10,\n","            isValSet_bool=True,\n","            contextSlices_count=3,\n","        )\n","\n","        batch_size = self.cli_args.batch_size\n","        if self.use_cuda:\n","            batch_size *= torch.cuda.device_count()\n","\n","        val_dl = DataLoader(\n","            val_ds,\n","            batch_size=batch_size,\n","            num_workers=self.cli_args.num_workers,\n","            pin_memory=self.use_cuda,\n","        )\n","\n","        return val_dl\n","    \n","    def initTensorboardWriters(self):\n","        if self.trn_writer is None:\n","            log_dir = os.path.join('runs', self.cli_args.tb_prefix, self.time_str)\n","\n","            self.trn_writer = SummaryWriter(\n","                log_dir=log_dir + '_trn_seg_' + self.cli_args.comment)\n","            self.val_writer = SummaryWriter(\n","                log_dir=log_dir + '_val_seg_' + self.cli_args.comment)\n","            \n","    def main(self):\n","        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n","\n","        train_dl = self.initTrainDl()\n","        val_dl = self.initValDl()\n","        \n","        best_score = 0.0\n","        self.validation_cadence = 5\n","        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n","            log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n","                epoch_ndx,\n","                self.cli_args.epochs,\n","                len(train_dl),\n","                len(val_dl),\n","                self.cli_args.batch_size,\n","                (torch.cuda.device_count() if self.use_cuda else 1),\n","            ))\n","            \n","            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n","            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n","            \n","            if epoch_ndx == 1 or epoch_ndx % self.validation_cadence == 0:\n","                valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n","                score = self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n","                best_score = max(score, best_score)\n","                print(\"score: {}, best_score: {}\".format(score, best_score))\n","\n","                self.saveModel('seg', epoch_ndx, score == best_score)\n","\n","                self.logImages(epoch_ndx, 'trn', train_dl)\n","                self.logImages(epoch_ndx, 'val', val_dl)\n","\n","        self.trn_writer.close()\n","        self.val_writer.close()\n","            \n","    \n","    def doTraining(self, epoch_ndx, train_dl):\n","        trnMetrics_g = torch.zeros(METRICS_SIZE, len(train_dl.dataset), device=self.device)\n","        self.segmentation_model.train()\n","        train_dl.dataset.shuffleSamples()\n","        \n","        batch_iter = enumerateWithEstimate(\n","            train_dl,\n","            \"E{} Training\".format(epoch_ndx),\n","            start_ndx=train_dl.num_workers,\n","        )\n","        \n","        for batch_ndx, batch_tup in batch_iter:\n","            self.optimizer.zero_grad()\n","            \n","            loss_var = self.computeBatchLoss(batch_ndx, batch_tup, train_dl.batch_size, trnMetrics_g)\n","            loss_var.backward()\n","            \n","            self.optimizer.step()\n","            \n","        self.totalTrainingSamples_count += trnMetrics_g.size(1)\n","        \n","        return trnMetrics_g.to('cpu')\n","    \n","    def doValidation(self, epoch_ndx, val_dl):\n","        with torch.no_grad():\n","            valMetrics_g = torch.zeros(METRICS_SIZE, len(val_dl.dataset), device=self.device)\n","            self.segmentation_model.eval()\n","            \n","            batch_iter = enumerateWithEstimate(\n","                val_dl,\n","                \"E{} Validation \".format(epoch_ndx),\n","                start_ndx=val_dl.num_workers,\n","            )\n","            \n","            for batch_ndx, batch_tup in batch_iter:\n","                self.computeBatchLoss(batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n","        \n","        return valMetrics_g.to('cpu')\n","    \n","    def diceLoss(self, prediction_g, label_g, epsilon=1):\n","        diceLabel_g = label_g.sum(dim=[1,2,3])\n","        dicePrediction_g = prediction_g.sum(dim=[1,2,3])\n","        diceCorrect_g = (prediction_g * label_g).sum(dim=[1,2,3])\n","        \n","        diceRatio_g = (2 * diceCorrect_g + epsilon) / (dicePrediction_g + diceLabel_g + epsilon)\n","        \n","        return 1 - diceRatio_g\n","    \n","    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g, \n","                         classificationThreshold=0.5):\n","        input_t, label_t, series_list, _slice_ndx_list = batch_tup\n","        \n","        input_g = input_t.to(self.device, non_blocking=True)\n","        label_g = label_t.to(self.device, non_blocking=True)\n","        \n","        if self.segmentation_model.training and self.augmentation_dict:\n","            input_g, label_g = self.augmentation_model(input_g, label_g)\n","            \n","        prediction_g = self.segmentation_model(input_g)\n","        \n","        diceLoss_g = self.diceLoss(prediction_g, label_g)\n","        fnLoss_g = self.diceLoss(prediction_g * label_g, label_g)\n","        \n","        start_ndx = batch_ndx * batch_size\n","        end_ndx = start_ndx + input_t.size(0)\n","        \n","        with torch.no_grad():\n","            predictionBool_g = (prediction_g[:, 0:1]\n","                                > classificationThreshold).to(torch.float32)\n","\n","            tp = (     predictionBool_g *  label_g).sum(dim=[1,2,3])\n","            fn = ((1 - predictionBool_g) *  label_g).sum(dim=[1,2,3])\n","            fp = (     predictionBool_g * (~label_g)).sum(dim=[1,2,3])\n","\n","            metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = diceLoss_g\n","            metrics_g[METRICS_TP_NDX, start_ndx:end_ndx] = tp\n","            metrics_g[METRICS_FN_NDX, start_ndx:end_ndx] = fn\n","            metrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp\n","            \n","        return diceLoss_g.mean() + fnLoss_g.mean() * 8\n","    \n","    def logImages(self, epoch_ndx, mode_str, dl):\n","        self.segmentation_model.eval()\n","\n","        images = sorted(dl.dataset.series_list)[:12]\n","        for series_ndx, series_uid in enumerate(images):\n","            ct = getCt(series_uid)\n","\n","            for slice_ndx in range(6):\n","                ct_ndx = slice_ndx * (ct.hu_a.shape[0] - 1) // 5\n","                sample_tup = dl.dataset.getitem_fullSlice(series_uid, ct_ndx)\n","\n","                ct_t, label_t, series_uid, ct_ndx = sample_tup\n","\n","                input_g = ct_t.to(self.device).unsqueeze(0)\n","                label_g = pos_g = label_t.to(self.device).unsqueeze(0)\n","\n","                prediction_g = self.segmentation_model(input_g)[0]\n","                prediction_a = prediction_g.to('cpu').detach().numpy()[0] > 0.5\n","                label_a = label_g.cpu().numpy()[0][0] > 0.5\n","\n","                ct_t[:-1,:,:] /= 2000\n","                ct_t[:-1,:,:] += 0.5\n","\n","                ctSlice_a = ct_t[dl.dataset.contextSlices_count].numpy()\n","\n","                image_a = np.zeros((512, 512, 3), dtype=np.float32)\n","                image_a[:,:,:] = ctSlice_a.reshape((512,512,1))\n","                image_a[:,:,0] += prediction_a & (1 - label_a)\n","                image_a[:,:,0] += (1 - prediction_a) & label_a\n","                image_a[:,:,1] += ((1 - prediction_a) & label_a) * 0.5\n","\n","                image_a[:,:,1] += prediction_a & label_a\n","                image_a *= 0.5\n","                image_a.clip(0, 1, image_a)\n","\n","                writer = getattr(self, mode_str + '_writer')\n","                writer.add_image(\n","                    f'{mode_str}/{series_ndx}_prediction_{slice_ndx}',\n","                    image_a,\n","                    self.totalTrainingSamples_count,\n","                    dataformats='HWC',\n","                )\n","\n","                if epoch_ndx == 1:\n","                    image_a = np.zeros((512, 512, 3), dtype=np.float32)\n","                    image_a[:,:,:] = ctSlice_a.reshape((512,512,1))\n","                    # image_a[:,:,0] += (1 - label_a) & lung_a # Red\n","                    image_a[:,:,1] += label_a  # Green\n","                    # image_a[:,:,2] += neg_a  # Blue\n","\n","                    image_a *= 0.5\n","                    image_a[image_a < 0] = 0\n","                    image_a[image_a > 1] = 1\n","                    writer.add_image(\n","                        '{}/{}_label_{}'.format(\n","                            mode_str,\n","                            series_ndx,\n","                            slice_ndx,\n","                        ),\n","                        image_a,\n","                        self.totalTrainingSamples_count,\n","                        dataformats='HWC',\n","                    )\n","                # This flush prevents TB from getting confused about which\n","                # data item belongs where.\n","                writer.flush()\n","                \n","    def logMetrics(self, epoch_ndx, mode_str, metrics_t):\n","        log.info(\"E{} {}\".format(\n","            epoch_ndx,\n","            type(self).__name__,\n","        ))\n","\n","        metrics_a = metrics_t.detach().numpy()\n","        sum_a = metrics_a.sum(axis=1)\n","        assert np.isfinite(metrics_a).all()\n","\n","        allLabel_count = sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]\n","\n","        metrics_dict = {}\n","        metrics_dict['loss/all'] = metrics_a[METRICS_LOSS_NDX].mean()\n","\n","        metrics_dict['percent_all/tp'] = \\\n","            sum_a[METRICS_TP_NDX] / (allLabel_count or 1) * 100\n","        metrics_dict['percent_all/fn'] = \\\n","            sum_a[METRICS_FN_NDX] / (allLabel_count or 1) * 100\n","        metrics_dict['percent_all/fp'] = \\\n","            sum_a[METRICS_FP_NDX] / (allLabel_count or 1) * 100\n","\n","\n","        precision = metrics_dict['pr/precision'] = sum_a[METRICS_TP_NDX] \\\n","            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FP_NDX]) or 1)\n","        recall    = metrics_dict['pr/recall']    = sum_a[METRICS_TP_NDX] \\\n","            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]) or 1)\n","\n","        metrics_dict['pr/f1_score'] = 2 * (precision * recall) \\\n","            / ((precision + recall) or 1)\n","\n","        log.info((\"E{} {:8} \"\n","                 + \"{loss/all:.4f} loss, \"\n","                 + \"{pr/precision:.4f} precision, \"\n","                 + \"{pr/recall:.4f} recall, \"\n","                 + \"{pr/f1_score:.4f} f1 score\"\n","                  ).format(\n","            epoch_ndx,\n","            mode_str,\n","            **metrics_dict,\n","        ))\n","        log.info((\"E{} {:8} \"\n","                  + \"{loss/all:.4f} loss, \"\n","                  + \"{percent_all/tp:-5.1f}% tp, {percent_all/fn:-5.1f}% fn, {percent_all/fp:-9.1f}% fp\"\n","        ).format(\n","            epoch_ndx,\n","            mode_str + '_all',\n","            **metrics_dict,\n","        ))\n","\n","        self.initTensorboardWriters()\n","        writer = getattr(self, mode_str + '_writer')\n","\n","        prefix_str = 'seg_'\n","\n","        for key, value in metrics_dict.items():\n","            writer.add_scalar(prefix_str + key, value, self.totalTrainingSamples_count)\n","\n","        writer.flush()\n","\n","        score = metrics_dict['pr/recall']\n","\n","        return score\n","    \n","    def saveModel(self, type_str, epoch_ndx, isBest=False):\n","        file_path = os.path.join(\n","            'models',\n","            self.cli_args.tb_prefix,\n","            '{}_{}_{}.{}.state'.format(\n","                type_str,\n","                self.time_str,\n","                self.cli_args.comment,\n","                self.totalTrainingSamples_count,\n","            )\n","        )\n","        \n","        os.makedirs(os.path.dirname(file_path), mode=0o755, exist_ok=True)\n","        \n","        model = self.segmentation_model\n","        if isinstance(model, torch.nn.DataParallel):\n","            model = model.module\n","            \n","        state = {\n","            'sys_argv': sys.argv,\n","            'time': str(datetime.datetime.now()),\n","            'model_state': model.state_dict(),\n","            'model_name': type(model).__name__,\n","            'optimizer_state' : self.optimizer.state_dict(),\n","            'optimizer_name': type(self.optimizer).__name__,\n","            'epoch': epoch_ndx,\n","            'totalTrainingSamples_count': self.totalTrainingSamples_count,\n","        }\n","        \n","        torch.save(state, file_path)\n","        \n","        log.info(\"Saved model params to {}\".format(file_path))\n","        \n","        if isBest:\n","            best_path = os.path.join(\n","                'models',\n","                self.cli_args.tb_prefix,\n","                f'{type_str}_{self.time_str}_{self.cli_args.comment}.best.state')\n","            shutil.copyfile(file_path, best_path)\n","\n","            log.info(\"Saved model params to {}\".format(best_path))\n","            \n","        with open(file_path, 'rb') as f:\n","            log.info(\"SHA1: \" + hashlib.sha1(f.read()).hexdigest())\n","        \n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Start Training"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T05:46:41.089107Z","iopub.status.busy":"2024-07-05T05:46:41.088696Z","iopub.status.idle":"2024-07-05T05:46:41.110212Z","shell.execute_reply":"2024-07-05T05:46:41.109491Z","shell.execute_reply.started":"2024-07-05T05:46:41.089073Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-05 05:46:41,102 INFO     pid:34 __main__:096:initModel Using CUDA; 2 devices.\n"]}],"source":["seg_app = SegmentationTrainingApp(['--epochs', '10', '--augmented'])"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T05:46:42.355797Z","iopub.status.busy":"2024-07-05T05:46:42.355044Z","iopub.status.idle":"2024-07-05T05:52:39.836182Z","shell.execute_reply":"2024-07-05T05:52:39.835239Z","shell.execute_reply.started":"2024-07-05T05:46:42.355762Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-05 05:46:42,356 INFO     pid:34 __main__:158:main Starting SegmentationTrainingApp, Namespace(batch_size=16, num_workers=4, epochs=10, augmented=True, augment_flip=False, augment_offset=False, augment_scale=False, augment_rotate=False, augment_noise=False, tb_prefix='seg', comment='none')\n","2024-07-05 05:46:42,444 INFO     pid:34 __main__:038:__init__ <__main__.TrainingLuna2dSegmentationDataset object at 0x7c516b297460>: 352 training series, 5212 slices, 644 nodules\n","2024-07-05 05:46:42,462 INFO     pid:34 __main__:038:__init__ <__main__.Luna2dSegmentationDataset object at 0x7c516ab84640>: 40 validation series, 616 slices, 76 nodules\n","2024-07-05 05:46:42,463 INFO     pid:34 __main__:166:main Epoch 1 of 10, 313/20 batches of size 16*2\n","2024-07-05 05:46:42,502 WARNING  pid:34 __main__:081:enumerateWithEstimate E1 Training ----/313, starting\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","2024-07-05 05:46:43,785 INFO     pid:34 __main__:098:enumerateWithEstimate E1 Training   16/313, done at 2024-07-05 05:47:00, 0:00:16\n","2024-07-05 05:46:46,350 INFO     pid:34 __main__:098:enumerateWithEstimate E1 Training   64/313, done at 2024-07-05 05:46:59, 0:00:16\n","2024-07-05 05:46:56,683 INFO     pid:34 __main__:098:enumerateWithEstimate E1 Training  256/313, done at 2024-07-05 05:46:59, 0:00:16\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","2024-07-05 05:46:59,753 WARNING  pid:34 __main__:111:enumerateWithEstimate E1 Training ----/313, done at 2024-07-05 05:46:59\n","2024-07-05 05:46:59,754 INFO     pid:34 __main__:342:logMetrics E1 SegmentationTrainingApp\n","2024-07-05 05:46:59,756 INFO     pid:34 __main__:372:logMetrics E1 trn      0.8242 loss, 0.1007 precision, 0.9870 recall, 0.1828 f1 score\n","2024-07-05 05:46:59,757 INFO     pid:34 __main__:382:logMetrics E1 trn_all  0.8242 loss,  98.7% tp,   1.3% fn,     881.0% fp\n","2024-07-05 05:46:59,768 WARNING  pid:34 __main__:081:enumerateWithEstimate E1 Validation  ----/20, starting\n","2024-07-05 05:47:15,407 INFO     pid:34 __main__:098:enumerateWithEstimate E1 Validation     8/20, done at 2024-07-05 05:47:31, 0:00:24\n","2024-07-05 05:47:18,844 INFO     pid:34 __main__:098:enumerateWithEstimate E1 Validation    16/20, done at 2024-07-05 05:47:21, 0:00:13\n","2024-07-05 05:47:19,717 WARNING  pid:34 __main__:111:enumerateWithEstimate E1 Validation  ----/20, done at 2024-07-05 05:47:19\n","2024-07-05 05:47:19,925 INFO     pid:34 __main__:342:logMetrics E1 SegmentationTrainingApp\n","2024-07-05 05:47:19,927 INFO     pid:34 __main__:372:logMetrics E1 val      0.9970 loss, 0.0015 precision, 0.9832 recall, 0.0029 f1 score\n","2024-07-05 05:47:19,928 INFO     pid:34 __main__:382:logMetrics E1 val_all  0.9970 loss,  98.3% tp,   1.7% fn,   66908.2% fp\n","2024-07-05 05:47:19,954 INFO     pid:34 __main__:436:saveModel Saved model params to models/seg/seg_2024-07-05_05.46.41_none.10000.state\n","2024-07-05 05:47:19,957 INFO     pid:34 __main__:445:saveModel Saved model params to models/seg/seg_2024-07-05_05.46.41_none.best.state\n","2024-07-05 05:47:19,960 INFO     pid:34 __main__:448:saveModel SHA1: fb18e41a80f2da26dfa3efe4da0b854af6e8622b\n"]},{"name":"stdout","output_type":"stream","text":["score: 0.9832338094711304, best_score: 0.9832338094711304\n"]},{"name":"stderr","output_type":"stream","text":["2024-07-05 05:48:15,995 INFO     pid:34 __main__:166:main Epoch 2 of 10, 313/20 batches of size 16*2\n","2024-07-05 05:48:16,038 WARNING  pid:34 __main__:081:enumerateWithEstimate E2 Training ----/313, starting\n","2024-07-05 05:48:17,302 INFO     pid:34 __main__:098:enumerateWithEstimate E2 Training   16/313, done at 2024-07-05 05:48:33, 0:00:17\n","2024-07-05 05:48:19,872 INFO     pid:34 __main__:098:enumerateWithEstimate E2 Training   64/313, done at 2024-07-05 05:48:33, 0:00:16\n","2024-07-05 05:48:30,167 INFO     pid:34 __main__:098:enumerateWithEstimate E2 Training  256/313, done at 2024-07-05 05:48:33, 0:00:16\n","2024-07-05 05:48:33,082 WARNING  pid:34 __main__:111:enumerateWithEstimate E2 Training ----/313, done at 2024-07-05 05:48:33\n","2024-07-05 05:48:33,084 INFO     pid:34 __main__:342:logMetrics E2 SegmentationTrainingApp\n","2024-07-05 05:48:33,085 INFO     pid:34 __main__:372:logMetrics E2 trn      0.7873 loss, 0.1180 precision, 0.9916 recall, 0.2108 f1 score\n","2024-07-05 05:48:33,086 INFO     pid:34 __main__:382:logMetrics E2 trn_all  0.7873 loss,  99.2% tp,   0.8% fn,     741.5% fp\n","2024-07-05 05:48:33,089 INFO     pid:34 __main__:166:main Epoch 3 of 10, 313/20 batches of size 16*2\n","2024-07-05 05:48:33,132 WARNING  pid:34 __main__:081:enumerateWithEstimate E3 Training ----/313, starting\n","2024-07-05 05:48:34,358 INFO     pid:34 __main__:098:enumerateWithEstimate E3 Training   16/313, done at 2024-07-05 05:48:50, 0:00:16\n","2024-07-05 05:48:37,180 INFO     pid:34 __main__:098:enumerateWithEstimate E3 Training   64/313, done at 2024-07-05 05:48:51, 0:00:17\n","2024-07-05 05:48:47,585 INFO     pid:34 __main__:098:enumerateWithEstimate E3 Training  256/313, done at 2024-07-05 05:48:50, 0:00:17\n","2024-07-05 05:48:50,494 WARNING  pid:34 __main__:111:enumerateWithEstimate E3 Training ----/313, done at 2024-07-05 05:48:50\n","2024-07-05 05:48:50,497 INFO     pid:34 __main__:342:logMetrics E3 SegmentationTrainingApp\n","2024-07-05 05:48:50,498 INFO     pid:34 __main__:372:logMetrics E3 trn      0.7274 loss, 0.1549 precision, 0.9683 recall, 0.2671 f1 score\n","2024-07-05 05:48:50,499 INFO     pid:34 __main__:382:logMetrics E3 trn_all  0.7274 loss,  96.8% tp,   3.2% fn,     528.1% fp\n","2024-07-05 05:48:50,501 INFO     pid:34 __main__:166:main Epoch 4 of 10, 313/20 batches of size 16*2\n","2024-07-05 05:48:50,545 WARNING  pid:34 __main__:081:enumerateWithEstimate E4 Training ----/313, starting\n","2024-07-05 05:48:51,769 INFO     pid:34 __main__:098:enumerateWithEstimate E4 Training   16/313, done at 2024-07-05 05:49:07, 0:00:16\n","2024-07-05 05:48:54,353 INFO     pid:34 __main__:098:enumerateWithEstimate E4 Training   64/313, done at 2024-07-05 05:49:07, 0:00:16\n","2024-07-05 05:49:04,695 INFO     pid:34 __main__:098:enumerateWithEstimate E4 Training  256/313, done at 2024-07-05 05:49:07, 0:00:16\n","2024-07-05 05:49:07,830 WARNING  pid:34 __main__:111:enumerateWithEstimate E4 Training ----/313, done at 2024-07-05 05:49:07\n","2024-07-05 05:49:07,832 INFO     pid:34 __main__:342:logMetrics E4 SegmentationTrainingApp\n","2024-07-05 05:49:07,834 INFO     pid:34 __main__:372:logMetrics E4 trn      0.7357 loss, 0.1441 precision, 0.9715 recall, 0.2509 f1 score\n","2024-07-05 05:49:07,835 INFO     pid:34 __main__:382:logMetrics E4 trn_all  0.7357 loss,  97.2% tp,   2.8% fn,     577.2% fp\n","2024-07-05 05:49:07,837 INFO     pid:34 __main__:166:main Epoch 5 of 10, 313/20 batches of size 16*2\n","2024-07-05 05:49:07,891 WARNING  pid:34 __main__:081:enumerateWithEstimate E5 Training ----/313, starting\n","2024-07-05 05:49:09,160 INFO     pid:34 __main__:098:enumerateWithEstimate E5 Training   16/313, done at 2024-07-05 05:49:24, 0:00:16\n","2024-07-05 05:49:11,701 INFO     pid:34 __main__:098:enumerateWithEstimate E5 Training   64/313, done at 2024-07-05 05:49:24, 0:00:16\n","2024-07-05 05:49:21,911 INFO     pid:34 __main__:098:enumerateWithEstimate E5 Training  256/313, done at 2024-07-05 05:49:24, 0:00:16\n","2024-07-05 05:49:24,790 WARNING  pid:34 __main__:111:enumerateWithEstimate E5 Training ----/313, done at 2024-07-05 05:49:24\n","2024-07-05 05:49:24,792 INFO     pid:34 __main__:342:logMetrics E5 SegmentationTrainingApp\n","2024-07-05 05:49:24,794 INFO     pid:34 __main__:372:logMetrics E5 trn      0.6215 loss, 0.2278 precision, 0.9439 recall, 0.3670 f1 score\n","2024-07-05 05:49:24,795 INFO     pid:34 __main__:382:logMetrics E5 trn_all  0.6215 loss,  94.4% tp,   5.6% fn,     319.9% fp\n","2024-07-05 05:49:24,798 WARNING  pid:34 __main__:081:enumerateWithEstimate E5 Validation  ----/20, starting\n","2024-07-05 05:49:42,514 INFO     pid:34 __main__:098:enumerateWithEstimate E5 Validation     8/20, done at 2024-07-05 05:50:09, 0:00:39\n","2024-07-05 05:49:46,712 INFO     pid:34 __main__:098:enumerateWithEstimate E5 Validation    16/20, done at 2024-07-05 05:49:50, 0:00:20\n","2024-07-05 05:49:46,973 WARNING  pid:34 __main__:111:enumerateWithEstimate E5 Validation  ----/20, done at 2024-07-05 05:49:46\n","2024-07-05 05:49:47,214 INFO     pid:34 __main__:342:logMetrics E5 SegmentationTrainingApp\n","2024-07-05 05:49:47,215 INFO     pid:34 __main__:372:logMetrics E5 val      0.9903 loss, 0.0044 precision, 0.9059 recall, 0.0088 f1 score\n","2024-07-05 05:49:47,216 INFO     pid:34 __main__:382:logMetrics E5 val_all  0.9903 loss,  90.6% tp,   9.4% fn,   20442.3% fp\n","2024-07-05 05:49:47,241 INFO     pid:34 __main__:436:saveModel Saved model params to models/seg/seg_2024-07-05_05.46.41_none.50000.state\n","2024-07-05 05:49:47,245 INFO     pid:34 __main__:448:saveModel SHA1: 1ea8023f08f3de5cacc933c5fc0b186b67726867\n"]},{"name":"stdout","output_type":"stream","text":["score: 0.9058927893638611, best_score: 0.9832338094711304\n"]},{"name":"stderr","output_type":"stream","text":["2024-07-05 05:50:22,219 INFO     pid:34 __main__:166:main Epoch 6 of 10, 313/20 batches of size 16*2\n","2024-07-05 05:50:22,263 WARNING  pid:34 __main__:081:enumerateWithEstimate E6 Training ----/313, starting\n","2024-07-05 05:50:23,496 INFO     pid:34 __main__:098:enumerateWithEstimate E6 Training   16/313, done at 2024-07-05 05:50:39, 0:00:16\n","2024-07-05 05:50:26,015 INFO     pid:34 __main__:098:enumerateWithEstimate E6 Training   64/313, done at 2024-07-05 05:50:39, 0:00:16\n","2024-07-05 05:50:36,171 INFO     pid:34 __main__:098:enumerateWithEstimate E6 Training  256/313, done at 2024-07-05 05:50:39, 0:00:16\n","2024-07-05 05:50:39,076 WARNING  pid:34 __main__:111:enumerateWithEstimate E6 Training ----/313, done at 2024-07-05 05:50:39\n","2024-07-05 05:50:39,078 INFO     pid:34 __main__:342:logMetrics E6 SegmentationTrainingApp\n","2024-07-05 05:50:39,080 INFO     pid:34 __main__:372:logMetrics E6 trn      0.5564 loss, 0.2608 precision, 0.9403 recall, 0.4083 f1 score\n","2024-07-05 05:50:39,080 INFO     pid:34 __main__:382:logMetrics E6 trn_all  0.5564 loss,  94.0% tp,   6.0% fn,     266.5% fp\n","2024-07-05 05:50:39,083 INFO     pid:34 __main__:166:main Epoch 7 of 10, 313/20 batches of size 16*2\n","2024-07-05 05:50:39,126 WARNING  pid:34 __main__:081:enumerateWithEstimate E7 Training ----/313, starting\n","2024-07-05 05:50:40,380 INFO     pid:34 __main__:098:enumerateWithEstimate E7 Training   16/313, done at 2024-07-05 05:50:56, 0:00:16\n","2024-07-05 05:50:43,179 INFO     pid:34 __main__:098:enumerateWithEstimate E7 Training   64/313, done at 2024-07-05 05:50:57, 0:00:17\n","2024-07-05 05:50:53,321 INFO     pid:34 __main__:098:enumerateWithEstimate E7 Training  256/313, done at 2024-07-05 05:50:56, 0:00:16\n","2024-07-05 05:50:56,313 WARNING  pid:34 __main__:111:enumerateWithEstimate E7 Training ----/313, done at 2024-07-05 05:50:56\n","2024-07-05 05:50:56,315 INFO     pid:34 __main__:342:logMetrics E7 SegmentationTrainingApp\n","2024-07-05 05:50:56,317 INFO     pid:34 __main__:372:logMetrics E7 trn      0.5041 loss, 0.2896 precision, 0.9363 recall, 0.4424 f1 score\n","2024-07-05 05:50:56,317 INFO     pid:34 __main__:382:logMetrics E7 trn_all  0.5041 loss,  93.6% tp,   6.4% fn,     229.7% fp\n","2024-07-05 05:50:56,320 INFO     pid:34 __main__:166:main Epoch 8 of 10, 313/20 batches of size 16*2\n","2024-07-05 05:50:56,364 WARNING  pid:34 __main__:081:enumerateWithEstimate E8 Training ----/313, starting\n","2024-07-05 05:50:57,585 INFO     pid:34 __main__:098:enumerateWithEstimate E8 Training   16/313, done at 2024-07-05 05:51:13, 0:00:16\n","2024-07-05 05:51:00,158 INFO     pid:34 __main__:098:enumerateWithEstimate E8 Training   64/313, done at 2024-07-05 05:51:13, 0:00:16\n","2024-07-05 05:51:10,397 INFO     pid:34 __main__:098:enumerateWithEstimate E8 Training  256/313, done at 2024-07-05 05:51:13, 0:00:16\n","2024-07-05 05:51:13,415 WARNING  pid:34 __main__:111:enumerateWithEstimate E8 Training ----/313, done at 2024-07-05 05:51:13\n","2024-07-05 05:51:13,417 INFO     pid:34 __main__:342:logMetrics E8 SegmentationTrainingApp\n","2024-07-05 05:51:13,418 INFO     pid:34 __main__:372:logMetrics E8 trn      0.4765 loss, 0.3093 precision, 0.9344 recall, 0.4647 f1 score\n","2024-07-05 05:51:13,425 INFO     pid:34 __main__:382:logMetrics E8 trn_all  0.4765 loss,  93.4% tp,   6.6% fn,     208.7% fp\n","2024-07-05 05:51:13,427 INFO     pid:34 __main__:166:main Epoch 9 of 10, 313/20 batches of size 16*2\n","2024-07-05 05:51:13,481 WARNING  pid:34 __main__:081:enumerateWithEstimate E9 Training ----/313, starting\n","2024-07-05 05:51:14,856 INFO     pid:34 __main__:098:enumerateWithEstimate E9 Training   16/313, done at 2024-07-05 05:51:32, 0:00:18\n","2024-07-05 05:51:17,415 INFO     pid:34 __main__:098:enumerateWithEstimate E9 Training   64/313, done at 2024-07-05 05:51:30, 0:00:16\n","2024-07-05 05:51:27,646 INFO     pid:34 __main__:098:enumerateWithEstimate E9 Training  256/313, done at 2024-07-05 05:51:30, 0:00:16\n","2024-07-05 05:51:30,575 WARNING  pid:34 __main__:111:enumerateWithEstimate E9 Training ----/313, done at 2024-07-05 05:51:30\n","2024-07-05 05:51:30,577 INFO     pid:34 __main__:342:logMetrics E9 SegmentationTrainingApp\n","2024-07-05 05:51:30,579 INFO     pid:34 __main__:372:logMetrics E9 trn      0.4746 loss, 0.3133 precision, 0.9357 recall, 0.4694 f1 score\n","2024-07-05 05:51:30,580 INFO     pid:34 __main__:382:logMetrics E9 trn_all  0.4746 loss,  93.6% tp,   6.4% fn,     205.1% fp\n","2024-07-05 05:51:30,582 INFO     pid:34 __main__:166:main Epoch 10 of 10, 313/20 batches of size 16*2\n","2024-07-05 05:51:30,627 WARNING  pid:34 __main__:081:enumerateWithEstimate E10 Training ----/313, starting\n","2024-07-05 05:51:31,834 INFO     pid:34 __main__:098:enumerateWithEstimate E10 Training   16/313, done at 2024-07-05 05:51:47, 0:00:16\n","2024-07-05 05:51:34,897 INFO     pid:34 __main__:098:enumerateWithEstimate E10 Training   64/313, done at 2024-07-05 05:51:50, 0:00:19\n","2024-07-05 05:51:45,210 INFO     pid:34 __main__:098:enumerateWithEstimate E10 Training  256/313, done at 2024-07-05 05:51:48, 0:00:17\n","2024-07-05 05:51:48,232 WARNING  pid:34 __main__:111:enumerateWithEstimate E10 Training ----/313, done at 2024-07-05 05:51:48\n","2024-07-05 05:51:48,234 INFO     pid:34 __main__:342:logMetrics E10 SegmentationTrainingApp\n","2024-07-05 05:51:48,235 INFO     pid:34 __main__:372:logMetrics E10 trn      0.4429 loss, 0.3441 precision, 0.9299 recall, 0.5023 f1 score\n","2024-07-05 05:51:48,236 INFO     pid:34 __main__:382:logMetrics E10 trn_all  0.4429 loss,  93.0% tp,   7.0% fn,     177.2% fp\n","2024-07-05 05:51:48,239 WARNING  pid:34 __main__:081:enumerateWithEstimate E10 Validation  ----/20, starting\n","2024-07-05 05:52:00,863 INFO     pid:34 __main__:098:enumerateWithEstimate E10 Validation     8/20, done at 2024-07-05 05:52:17, 0:00:23\n","2024-07-05 05:52:04,744 INFO     pid:34 __main__:098:enumerateWithEstimate E10 Validation    16/20, done at 2024-07-05 05:52:07, 0:00:13\n","2024-07-05 05:52:04,919 WARNING  pid:34 __main__:111:enumerateWithEstimate E10 Validation  ----/20, done at 2024-07-05 05:52:04\n","2024-07-05 05:52:05,266 INFO     pid:34 __main__:342:logMetrics E10 SegmentationTrainingApp\n","2024-07-05 05:52:05,268 INFO     pid:34 __main__:372:logMetrics E10 val      0.9879 loss, 0.0052 precision, 0.9298 recall, 0.0104 f1 score\n","2024-07-05 05:52:05,269 INFO     pid:34 __main__:382:logMetrics E10 val_all  0.9879 loss,  93.0% tp,   7.0% fn,   17770.5% fp\n","2024-07-05 05:52:05,295 INFO     pid:34 __main__:436:saveModel Saved model params to models/seg/seg_2024-07-05_05.46.41_none.100000.state\n","2024-07-05 05:52:05,298 INFO     pid:34 __main__:448:saveModel SHA1: 71cb3d1af8c5d87553c5b99d406a491f217a7585\n"]},{"name":"stdout","output_type":"stream","text":["score: 0.9297932982444763, best_score: 0.9832338094711304\n"]}],"source":["seg_app.main()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2485074,"sourceId":4215673,"sourceType":"datasetVersion"},{"datasetId":5318757,"sourceId":8838232,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
